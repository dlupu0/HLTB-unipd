{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTj_MOZOmzfn"
   },
   "source": [
    "# HowLongToBeat RDF Creator\n",
    "\n",
    "We load the generated CSV files and we serialize all the data into ***turtle format  (TTL)*** relying on ***RDFLib*** Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPtFAXlZmzfp"
   },
   "source": [
    "## Setup\n",
    "\n",
    "We import all the necessary libraries and we set the paths to the input/output files. In particular, we create a TTL file for each type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "JXZwhL0Bmzfp"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "# RDFLib knows about some namespaces, like XSD\n",
    "from rdflib.namespace import XSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "absPath = str(Path(os.path.abspath(os.getcwd())).absolute())\n",
    "datasetsPath = os.path.join(absPath, \"cleaned_datasets\")\n",
    "rawdatasetsPath = os.path.join(absPath, \"raw_datasets\")\n",
    "rdfPath = os.path.join(absPath, \"rdf\")\n",
    "\n",
    "# Create dataset directory if not exists\n",
    "if not os.path.exists(datasetsPath):\n",
    "    os.mkdir(datasetsPath)\n",
    "\n",
    "# Create RDF directory if not exists\n",
    "if not os.path.exists(rdfPath):\n",
    "    os.mkdir(rdfPath)\n",
    "\n",
    "# Setup datasets paths\n",
    "gamesPath = os.path.join(datasetsPath, \"games_cleaned.csv\")\n",
    "vgchartzPath = os.path.join(datasetsPath, \"vgchartz_cleaned.csv\")\n",
    "indiegamesdevelopersPath = os.path.join(datasetsPath, \"indiegamesdevelopers_cleaned_seriesExplode.csv\")\n",
    "platformsPath = os.path.join(datasetsPath, \"platforms.csv\")\n",
    "videoGameDevelopersPath = os.path.join(datasetsPath, \"videogamesdevelopers_cleaned_seriesexplode.csv\")\n",
    "completionTimePath = os.path.join(datasetsPath, \"completion_time.csv\")\n",
    "\n",
    "# Setup raw datasets\n",
    "rawVGChartsPath = os.path.join(rawdatasetsPath, \"vgchartz-7_7_2020.csv\")\n",
    "rawGamesPath = os.path.join(rawdatasetsPath, \"games.csv\")\n",
    "rawCountriesRegionsPath = os.path.join(rawdatasetsPath,'countries-regions.csv')\n",
    "\n",
    "# Countries-Regions path\n",
    "countriesRegionsPath = os.path.join(datasetsPath, \"countries-regions.csv\")\n",
    "countriesPath = os.path.join(absPath, \"wikipedia-iso-country-codes.csv\")\n",
    "\n",
    "# Setup Turtle paths\n",
    "genresTTLPath = os.path.join(rdfPath, \"genres.ttl\")\n",
    "gamesTTLPath = os.path.join(rdfPath, \"games.ttl\")\n",
    "companyTTLPath = os.path.join(rdfPath, \"company.ttl\")\n",
    "platformsTTLPath = os.path.join(rdfPath, \"platforms.ttl\")\n",
    "platformsSalesTTLPath = os.path.join(rdfPath, \"platformsSales.ttl\")\n",
    "\n",
    "videoGameDevelopersTTLPath = os.path.join(rdfPath, \"videoGameDevelopers.ttl\")\n",
    "statsTTLPath = os.path.join(rdfPath, \"stats.ttl\")\n",
    "gameSalesTTLPath = os.path.join(rdfPath, \"gameSales.ttl\")\n",
    "regionsTTLPath = os.path.join(rdfPath, \"regions.ttl\")\n",
    "countriesTTLPath = os.path.join(rdfPath, \"countries.ttl\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# Country Ontology\n",
    "CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "\n",
    "# HLTB Ontology\n",
    "HLTB = Namespace(\"http://www.semanticweb.org/enrico/ontologies/2022/10/HLTB-db2unipd#\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "def createGraph():\n",
    "    # Create the graph\n",
    "    g = Graph()\n",
    "\n",
    "    # Bind the namespaces to a prefix for more readable output\n",
    "    g.bind(\"xsd\", XSD)\n",
    "    g.bind(\"countries\", CNS)\n",
    "    g.bind(\"hltb\", HLTB)\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "#create game URI\n",
    "def createGameID(title):\n",
    "    # Replace all special chars with \"-\"\n",
    "    gameID = \"\"\n",
    "    for char in title:\n",
    "        if char.isalnum():\n",
    "            gameID += char\n",
    "        elif len(gameID) > 0 and gameID[-1] != '-':\n",
    "            gameID += '-'\n",
    "    if len(gameID) > 0 and gameID[-1] == '-':\n",
    "        gameID = gameID[:-1]\n",
    "    #print(gameID.lower())\n",
    "    return gameID.lower()\n",
    "\n",
    "\n",
    "#Create genre URI\n",
    "def setGenreID(genre):  ##first half is the original genres, second half are processed and lowercase\n",
    "    genre = str(genre).replace(\"/\", \", \").replace(\"nan\", \"\")\n",
    "    genre = genre.split(\", \")\n",
    "    list = []\n",
    "    for i in range(len(genre)):\n",
    "        list.append([])\n",
    "        list[i].append(genre[i])\n",
    "        list[i].append(genre[i].lower().replace(\"'\", \"\").replace(\" \", \"-\"))\n",
    "    return (list)\n",
    "\n",
    "\n",
    "def setPlatformID(platform):\n",
    "    return platform.lower().replace(\" \", \"-\").replace(\"/\", \"-\").replace(\"&\", \"-\")\n",
    "\n",
    "\n",
    "def setCompanyID(company):\n",
    "    if ' ' in company:\n",
    "        return company.replace(' ', '-').lower()\n",
    "    return company.lower()\n",
    "\n",
    "\n",
    "def setDeveloperID(developer):\n",
    "    return developer.replace(' ', '-', regex=True).str.lower()\n",
    "\n",
    "\n",
    "def setCountryID(country):\n",
    "    return country.replace(' ', '-', regex=True).str.lower()\n",
    "\n",
    "\n",
    "def getCountry2Digits(country):\n",
    "\n",
    "    rawCountriesRegions = pd.read_csv(rawCountriesRegionsPath, sep=\",\", index_col='name')\n",
    "    if country in rawCountriesRegions.index:\n",
    "        return rawCountriesRegions.iloc[rawCountriesRegions.index==country]['alpha-2']\n",
    "        #return rawCountriesRegions[rawCountriesRegions.index==country]['alpha-2'].values\n",
    "    return ''\n",
    "\n",
    "        #return str(rawCountriesRegions[rawCountriesRegions.index==country]['alpha-2']).lower()\n",
    "\n",
    "    #if country.lower() == 'us' or country.lower() == 'united states of america':\n",
    "    #    return 'us'\n",
    "    #if country.lower() == 'South Korea':\n",
    "    #    return 'kr'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Serialization\n",
    "\n",
    "We serialize the data according to the following workflow:\n",
    "\n",
    "1. Load the CSV file and iterate through it\n",
    "2. Create a unique ID by ourself based on the name of the class.\n",
    "3. Add the node to the graph using the unique ID.\n",
    "4. Add all the data properties.\n",
    "5. Add all the object properties.\n",
    "6. Serialize the data and save them into a TTL file.\n",
    "\n",
    "### Games\n",
    "\n",
    "Now serializing the Game class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "games = pd.read_csv(gamesPath, sep=\",\", index_col=\"title\")\n",
    "vgchartz = pd.read_csv(vgchartzPath, sep=\",\")\n",
    "platforms = pd.read_csv(platformsPath, sep=\",\")\n",
    "\n",
    "merged = pd.merge(vgchartz, platforms, left_on='console', right_on='Acronym', how='left')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# Iterate over the games\n",
    "for title, row in games.iterrows():\n",
    "    # Create gameID from its title\n",
    "    gameID = createGameID(title)\n",
    "\n",
    "    # Create the node to add to the Graph\n",
    "    Game = URIRef(HLTB[gameID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Game, RDF.type, HLTB.Game))\n",
    "\n",
    "    # Add the title of the game\n",
    "    g.add((Game, HLTB[\"officialName\"], Literal(title, datatype=XSD.string)))\n",
    "\n",
    "    # Add multiplayer focus\n",
    "    g.add((Game, HLTB[\"multiplayerFocus\"],\n",
    "           Literal(pd.notnull(row[\"coop\"]) or pd.notnull(row[\"versus\"]), datatype=XSD.boolean)))\n",
    "\n",
    "    #Add hltb id\n",
    "    g.add((Game, HLTB[\"id\"], Literal(row[\"id\"], datatype=XSD.int)))\n",
    "\n",
    "    #Add hasGenre object property\n",
    "    for iterator in setGenreID(row[\"genres\"]):\n",
    "        if pd.notnull(iterator[1]) and iterator[1] != '':\n",
    "            g.add((Game, HLTB[\"hasGenre\"], URIRef(HLTB[iterator[1]])))\n",
    "\n",
    "    #Add platform availability\n",
    "    if pd.notna(row[\"platforms\"]):\n",
    "        for platform in row[\"platforms\"].split(\", \"):\n",
    "            g.add((Game, HLTB[\"releasedOn\"], URIRef(HLTB[setPlatformID(platform)])))\n",
    "\n",
    "            #Add Stats object property\n",
    "            g.add((Game, HLTB[\"hasStats\"],\n",
    "                   URIRef(HLTB[\"stats-\" + str(createGameID(title)) + \"___\" + str(setPlatformID(platform))])))\n",
    "\n",
    "            # Add Sales object property\n",
    "            game = merged.loc[(merged['title'] == title) & (merged['Platform'] == platform)]\n",
    "            if not game.empty:\n",
    "                if pd.notna(game[\"pal_sales\"].iloc[0]):\n",
    "                    GameSalesID = URIRef(\n",
    "                        HLTB[\"sales-\" + str(createGameID(title)) + \"___\" + str(setPlatformID(platform)) + \"___\" + \"eu\"])\n",
    "                    g.add((Game, HLTB[\"sold\"], GameSalesID))\n",
    "                if pd.notna(game[\"na_sales\"].iloc[0]):\n",
    "                    GameSalesID = URIRef(\n",
    "                        HLTB[\"sales-\" + str(createGameID(title)) + \"___\" + str(setPlatformID(platform)) + \"___\" + \"na\"])\n",
    "                    g.add((Game, HLTB[\"sold\"], GameSalesID))\n",
    "                if pd.notna(game[\"jp_sales\"].iloc[0]):\n",
    "                    GameSalesID = URIRef(\n",
    "                        HLTB[\"sales-\" + str(createGameID(title)) + \"___\" + str(setPlatformID(platform)) + \"___\" + \"jp\"])\n",
    "                    g.add((Game, HLTB[\"sold\"], GameSalesID))\n",
    "                if pd.notna(game[\"other_sales\"].iloc[0]):\n",
    "                    GameSalesID = URIRef(HLTB[\"sales-\" + str(createGameID(title)) + \"___\" + str(\n",
    "                        setPlatformID(platform)) + \"___\" + \"other\"])\n",
    "                    g.add((Game, HLTB[\"sold\"], GameSalesID))\n",
    "                if pd.notna(game[\"total_shipped\"].iloc[0]):\n",
    "                    GameSalesID = URIRef(HLTB[\"sales-\" + str(createGameID(title)) + \"___\" + str(\n",
    "                        setPlatformID(platform)) + \"___\" + \"global\"])\n",
    "                    g.add((Game, HLTB[\"sold\"], GameSalesID))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved games TTL file.\n"
     ]
    }
   ],
   "source": [
    "# Save the data in the Turtle format\n",
    "with open(gamesTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))\n",
    "\n",
    "print(\"Saved games TTL file.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Genre\n",
    "\n",
    "Now serializing the Genre class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()\n",
    "\n",
    "# Load the CSV files in memory\n",
    "genres = pd.read_csv(gamesPath, sep=\",\", index_col=\"genres\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "for genre, row in genres.iterrows():\n",
    "    allGenres = setGenreID(genre)\n",
    "    for iterator in allGenres:\n",
    "        if not (iterator[0] == \" \" or iterator[0] == \"\"):\n",
    "            Genre = URIRef(HLTB[iterator[1]])\n",
    "            #Add triples using store's add() method.\n",
    "            g.add((Genre, RDF.type, HLTB.Genre))\n",
    "            # Add the name of the genre\n",
    "            g.add((Genre, HLTB[\"name\"], Literal(iterator[0], datatype=XSD.string)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# Save genre data in the Turtle format\n",
    "with open(genresTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Platforms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()\n",
    "# Load the CSV files in memory\n",
    "platforms = pd.read_csv(platformsPath, sep=\",\", index_col=\"Platform\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "manufacturerDict = {}\n",
    "\n",
    "for platform, row in platforms.iterrows():\n",
    "    Platform = URIRef(HLTB[setPlatformID(platform)])\n",
    "    #Add triples using store's add() method.\n",
    "    g.add((Platform, RDF.type, HLTB.Platform))\n",
    "    # Add the name of the genre\n",
    "    g.add((Platform, HLTB[\"name\"], Literal(platform, datatype=XSD.string)))\n",
    "\n",
    "    #Add popularity if platform is popular\n",
    "    if row[\"Popular\"]:\n",
    "        g.add((Platform, HLTB[\"popular\"], Literal(True, datatype=XSD.boolean)))\n",
    "\n",
    "    #Add release date if present\n",
    "    if pd.notna(row[\"Release date\"]):\n",
    "        time = datetime.combine(datetime.strptime(row[\"Release date\"], '%Y-%M-%d'), datetime.min.time())\n",
    "        g.add((Platform, HLTB[\"releaseDate\"], Literal(time, datatype=XSD.dateTime)))\n",
    "\n",
    "    #Add CPU information\n",
    "    if pd.notna(row[\"CPU\"]):\n",
    "        g.add((Platform, HLTB[\"cpu\"], Literal(row[\"CPU\"], datatype=XSD.string)))\n",
    "\n",
    "    #Add CPU bit information\n",
    "    if pd.notna(row[\"\\\"Bits\\\"\"]):\n",
    "        bits = row[\"\\\"Bits\\\"\"].split(\"-\")[0]\n",
    "        g.add((Platform, HLTB[\"bits\"], Literal(bits, datatype=XSD.int)))\n",
    "\n",
    "    #Add acronym information\n",
    "    if pd.notna(row[\"Acronym\"]):\n",
    "        g.add((Platform, HLTB[\"acronym\"], Literal(row[\"Acronym\"], datatype=XSD.string)))\n",
    "\n",
    "    # add the manufacturer name\n",
    "    if pd.notna(row[\"Manufacturer\"]) or pd.notnull(row[\"Manufacturer\"]):\n",
    "        manufacturerStr = row[\"Manufacturer\"].strip()\n",
    "        if ',' in manufacturerStr:\n",
    "            manufacturerStr = manufacturerStr.replace(',', '/')\n",
    "        manufacturerSplit = manufacturerStr.split('/')\n",
    "        for elem in manufacturerSplit:\n",
    "            elem = elem.strip()\n",
    "            manufacturerName = ''\n",
    "            manufacturerCountry = ''\n",
    "            if '(' in elem:\n",
    "                elemSplit = elem.split('(')\n",
    "                manufacturerName = elemSplit[0].strip()\n",
    "                manufacturerCountry = elemSplit[1].strip()\n",
    "                if not manufacturerName[-1].isalnum():\n",
    "                    manufacturerName = manufacturerName[:-1]\n",
    "                if manufacturerCountry[-1] == ')':\n",
    "                    manufacturerCountry = manufacturerCountry[:-1]\n",
    "                manufacturerCountry = ''.join(c for c in manufacturerCountry if c.isalnum() or c == ' ')\n",
    "            else:\n",
    "                manufacturerName = elem\n",
    "                if manufacturerName == 'Panasonic' or manufacturerName == 'Sega':\n",
    "                    manufacturerCountry = 'Japan'\n",
    "\n",
    "            manufacturerDict[manufacturerName] = manufacturerCountry\n",
    "            #print(manufacturerCountry)\n",
    "            if manufacturerName != '':\n",
    "                manufacturer = URIRef(HLTB[setCompanyID(manufacturerName)])\n",
    "                g.add((Platform, HLTB[\"createdBy\"], manufacturer))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# Save data in the Turtle format\n",
    "with open(platformsTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stats\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "g = createGraph()\n",
    "games = pd.read_csv(gamesPath, sep=\",\")\n",
    "vgchartz = pd.read_csv(vgchartzPath, sep=\",\")\n",
    "completionTime = pd.read_csv(completionTimePath, sep=\",\")\n",
    "platforms = pd.read_csv(platformsPath, sep=\",\")\n",
    "\n",
    "merged_temp = pd.merge(games, completionTime, left_on='id',\n",
    "                       right_on='gameID')\n",
    "merged_temp1 = pd.merge(vgchartz, platforms, left_on='console', right_on='Acronym', how='left')\n",
    "merged = pd.merge(merged_temp, merged_temp1, left_on=[\"title\", \"platform\"], right_on=[\"title\", \"Platform\"], how='left')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "for id, row in merged.iterrows():\n",
    "    if pd.notna(row[\"title\"]) and pd.notna(row[\"platform\"]):\n",
    "        StatsID = URIRef(HLTB[createGameID(\"stats-\" + str(createGameID(row[\"title\"]))) + \"___\" + str(\n",
    "            setPlatformID(row[\"platform\"]))])\n",
    "\n",
    "        #Adding node type\n",
    "        g.add((StatsID, HLTB.Type, HLTB.Stats))\n",
    "\n",
    "        #Add Time information\n",
    "        g.add((StatsID, HLTB[\"polledTime\"], Literal(row[\"count_comp\"], datatype=XSD.int)))\n",
    "        g.add((StatsID, HLTB[\"mainTime\"], Literal(row[\"comp_main\"], datatype=XSD.int)))\n",
    "        g.add((StatsID, HLTB[\"mainPlusTime\"], Literal(row[\"comp_plus\"], datatype=XSD.int)))\n",
    "        g.add((StatsID, HLTB[\"completionistTime\"], Literal(row[\"comp_100\"], datatype=XSD.int)))\n",
    "        g.add((StatsID, HLTB[\"slowestTime\"], Literal(row[\"comp_low\"], datatype=XSD.int)))\n",
    "        g.add((StatsID, HLTB[\"fastestTime\"], Literal(row[\"comp_high\"], datatype=XSD.int)))\n",
    "\n",
    "        #Add remaining stats\n",
    "        if pd.notna(row[\"critic_score\"]):\n",
    "            g.add((StatsID, HLTB[\"criticScore\"], Literal(row[\"critic_score\"], datatype=XSD.float)))\n",
    "        if pd.notna(row[\"user_score\"]):\n",
    "            g.add((StatsID, HLTB[\"userScore\"], Literal(row[\"user_score\"], datatype=XSD.float)))\n",
    "\n",
    "        if pd.notna(row[\"release_date\"]):\n",
    "            time = datetime.combine(datetime.strptime(row[\"release_date\"], '%Y-%M-%d'), datetime.min.time())\n",
    "            g.add((StatsID, HLTB[\"releaseDate\"], Literal(time, datatype=XSD.dateTime)))\n",
    "\n",
    "        #Add \"onPlatform\" object property\n",
    "        g.add((StatsID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"platform\"])])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved company TTL file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the data in the Turtle format\n",
    "with open(statsTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))\n",
    "\n",
    "print(\"Saved company TTL file.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Game sales"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "g = createGraph()\n",
    "vgchartz = pd.read_csv(vgchartzPath, sep=\",\")\n",
    "platforms = pd.read_csv(platformsPath, sep=\",\")\n",
    "\n",
    "merged = pd.merge(vgchartz, platforms, left_on='console', right_on='Acronym', how='left')\n",
    "merged.to_csv(\"prova.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "for id, row in merged.iterrows():\n",
    "    if pd.notna(row[\"Platform\"]):\n",
    "\n",
    "        if pd.notna(row[\"pal_sales\"]):\n",
    "            GameSalesID = URIRef(HLTB[\"sales-\" + str(createGameID(row[\"title\"])) + \"___\" + str(\n",
    "                setPlatformID(row[\"Platform\"])) + \"___\" + \"eu\"])\n",
    "            g.add((GameSalesID, HLTB.Type, HLTB.Sale))\n",
    "            g.add((GameSalesID, HLTB[\"unitsSold\"], Literal(row[\"pal_sales\"], datatype=XSD.float)))\n",
    "            g.add((GameSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"eu\"])))\n",
    "            g.add((GameSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n",
    "\n",
    "        if pd.notna(row[\"jp_sales\"]):\n",
    "            GameSalesID = URIRef(HLTB[\"sales-\" + str(createGameID(row[\"title\"])) + \"___\" + str(\n",
    "                setPlatformID(row[\"Platform\"])) + \"___\" + \"jp\"])\n",
    "            g.add((GameSalesID, HLTB.Type, HLTB.Sale))\n",
    "            g.add((GameSalesID, HLTB[\"unitsSold\"], Literal(row[\"jp_sales\"], datatype=XSD.float)))\n",
    "            g.add((GameSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"jp\"])))\n",
    "            g.add((GameSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n",
    "\n",
    "        if pd.notna(row[\"na_sales\"]):\n",
    "            GameSalesID = URIRef(HLTB[\"sales-\" + str(createGameID(row[\"title\"])) + \"___\" + str(\n",
    "                setPlatformID(row[\"Platform\"])) + \"___\" + \"na\"])\n",
    "            g.add((GameSalesID, HLTB.Type, HLTB.Sale))\n",
    "            g.add((GameSalesID, HLTB[\"unitsSold\"], Literal(row[\"na_sales\"], datatype=XSD.float)))\n",
    "            g.add((GameSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"na\"])))\n",
    "            g.add((GameSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n",
    "\n",
    "        if pd.notna(row[\"other_sales\"]):\n",
    "            GameSalesID = URIRef(HLTB[\"sales-\" + str(createGameID(row[\"title\"])) + \"___\" + str(\n",
    "                setPlatformID(row[\"Platform\"])) + \"___\" + \"other\"])\n",
    "            g.add((GameSalesID, HLTB.Type, HLTB.Sale))\n",
    "            g.add((GameSalesID, HLTB[\"unitsSold\"], Literal(row[\"other_sales\"], datatype=XSD.float)))\n",
    "            g.add((GameSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"other\"])))\n",
    "            g.add((GameSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n",
    "\n",
    "        if pd.notna(row[\"total_shipped\"]):\n",
    "            GameSalesID = URIRef(HLTB[\"sales-\" + str(createGameID(row[\"title\"])) + \"___\" + str(\n",
    "                setPlatformID(row[\"Platform\"])) + \"___\" + \"global\"])\n",
    "            g.add((GameSalesID, HLTB.Type, HLTB.Sale))\n",
    "            g.add((GameSalesID, HLTB[\"unitsSold\"], Literal(row[\"total_shipped\"], datatype=XSD.float)))\n",
    "            g.add((GameSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"global\"])))\n",
    "            g.add((GameSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved game-sales TTL file.\n"
     ]
    }
   ],
   "source": [
    "# Save the data in the Turtle format\n",
    "with open(gameSalesTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))\n",
    "\n",
    "print(\"Saved game-sales TTL file.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Company"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()\n",
    "# Load the CSV files in memory\n",
    "\n",
    "indiegamesdevelopers = pd.read_csv(indiegamesdevelopersPath, sep=\",\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "#companies=[]\n",
    "#replace space with -, lower case\n",
    "indiegamesdevelopers['companyID'] = setDeveloperID(indiegamesdevelopers['Developer'])\n",
    "#indiegamesdevelopers['companyID'] = setCompanyID(indiegamesdevelopers['Developer'])\n",
    "\n",
    "\n",
    "#indiegamesdevelopers.info()\n",
    "for index, row in indiegamesdevelopers.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    Company = URIRef(HLTB[row['companyID']])\n",
    "    #companies.append(Company)\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Company, RDF.type, HLTB.Company))\n",
    "\n",
    "    # Add the Company\n",
    "    g.add((Company, HLTB[\"indieDeveloper\"], Literal(pd.notnull(row['Developer']), datatype=XSD.boolean)))\n",
    "    g.add((Company, HLTB['officialName'], Literal(row['Developer'], datatype=XSD.string)))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### video-game-developers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "videoGameDevelopers = pd.read_csv(videoGameDevelopersPath, sep=\",\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "#replace space with -, lower case\n",
    "videoGameDevelopers['companyID'] = setDeveloperID(videoGameDevelopers['Developer'])\n",
    "#videoGameDevelopers['companyID'] = setCompanyID(videoGameDevelopers['Developer'])\n",
    "\n",
    "\n",
    "#indiegamesdevelopers.info()\n",
    "for index, row in videoGameDevelopers.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    Company = URIRef(HLTB[row['companyID']])\n",
    "    #companies.append(Company)\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Company, RDF.type, HLTB.Company))\n",
    "\n",
    "    # Add the Company\n",
    "    g.add((Company, HLTB['officialName'], Literal(row['Developer'], datatype=XSD.string)))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japan\n",
      "Japan\n",
      "South Korea\n",
      "Canada\n",
      "US\n",
      "US\n",
      "US\n",
      "Japan\n",
      "Japan\n",
      "US\n",
      "Japan\n",
      "US\n",
      "Netherlands\n",
      "Japan\n",
      "Brazil\n",
      "Japan\n",
      "US\n",
      "US\n"
     ]
    }
   ],
   "source": [
    "for manufacturer, country in manufacturerDict.items():\n",
    "    manufacturerURI = URIRef(HLTB[setCompanyID(manufacturer)])\n",
    "    g.add((manufacturerURI, RDF.type, HLTB.Company))\n",
    "    g.add((manufacturerURI, HLTB['officialName'], Literal(manufacturer, datatype=XSD.string)))\n",
    "    # add country of that manufacturer\n",
    "    print(country)\n",
    "    #g.add((manufacturerURI, HLTB['basedIn'], )))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved company TTL file.\n"
     ]
    }
   ],
   "source": [
    "# Save the data in the Turtle format\n",
    "with open(companyTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))\n",
    "\n",
    "print(\"Saved company TTL file.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Region"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "<Graph identifier=N98c25f759f0947e0bb07665ebaeea017 (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the node to add to the Graph\n",
    "Region = URIRef(HLTB['eu'])\n",
    "g.add((Region, RDF.type, HLTB.Region))\n",
    "g.add((Region, HLTB[{'eu', 'jp', 'na', 'other', 'global'}], Literal('eu', datatype=XSD.string)))\n",
    "\n",
    "Region = URIRef(HLTB['jp'])\n",
    "g.add((Region, RDF.type, HLTB.Region))\n",
    "g.add((Region, HLTB[{'eu', 'jp', 'na', 'other', 'global'}], Literal('jp', datatype=XSD.string)))\n",
    "\n",
    "Region = URIRef(HLTB['na'])\n",
    "g.add((Region, RDF.type, HLTB.Region))\n",
    "g.add((Region, HLTB[{'eu', 'jp', 'na', 'other', 'global'}], Literal('na', datatype=XSD.string)))\n",
    "\n",
    "Region = URIRef(HLTB['other'])\n",
    "g.add((Region, RDF.type, HLTB.Region))\n",
    "g.add((Region, HLTB[{'eu', 'jp', 'na', 'other', 'global'}], Literal('other', datatype=XSD.string)))\n",
    "\n",
    "Region = URIRef(HLTB['global'])\n",
    "g.add((Region, RDF.type, HLTB.Region))\n",
    "g.add((Region, HLTB[{'eu', 'jp', 'na', 'other', 'global'}], Literal('global', datatype=XSD.string)))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Regions TTL file.\n"
     ]
    }
   ],
   "source": [
    "# Save the data in the Turtle format\n",
    "with open(regionsTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))\n",
    "\n",
    "print(\"Saved Regions TTL file.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Country"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japan name\n",
      "Japan    JP\n",
      "Name: alpha-2, dtype: object\n",
      "Japan name\n",
      "Japan    JP\n",
      "Name: alpha-2, dtype: object\n",
      "South Korea \n",
      "Canada name\n",
      "Canada    CA\n",
      "Name: alpha-2, dtype: object\n",
      "US \n",
      "US \n",
      "US \n",
      "Japan name\n",
      "Japan    JP\n",
      "Name: alpha-2, dtype: object\n",
      "Japan name\n",
      "Japan    JP\n",
      "Name: alpha-2, dtype: object\n",
      "US \n",
      "Japan name\n",
      "Japan    JP\n",
      "Name: alpha-2, dtype: object\n",
      "US \n",
      "Netherlands name\n",
      "Netherlands    NL\n",
      "Name: alpha-2, dtype: object\n",
      "Japan name\n",
      "Japan    JP\n",
      "Name: alpha-2, dtype: object\n",
      "Brazil name\n",
      "Brazil    BR\n",
      "Name: alpha-2, dtype: object\n",
      "Japan name\n",
      "Japan    JP\n",
      "Name: alpha-2, dtype: object\n",
      "US \n",
      "US \n"
     ]
    }
   ],
   "source": [
    "for val in manufacturerDict.values():\n",
    "    print(val, getCountry2Digits(val))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "countries = pd.read_csv(countriesPath, sep=',', index_col='Name', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "rawCountriesRegions = pd.read_csv(rawCountriesRegionsPath, sep=\",\", index_col='name')\n",
    "\n",
    "g = createGraph()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bolivia', 'Cape Verde', 'Congo, the Democratic Republic of the', 'Czech Republic', 'Holy See (Vatican City State)', 'Iran', \"Korea, Democratic People's Republic of\", 'Korea, Republic of (South Korea)', 'Libyan Arab Jamahiriya', 'Macedonia, the former Yugoslav Republic of', 'Micronesia, Federated States of', 'Netherlands Antilles', 'Palestinian Territory, Occupied', 'Reunion', 'Russia', 'Saint Martin', 'Swaziland', 'Taiwan', 'Turks and Caicos', 'United Kingdom', 'Venezuela', 'Vietnam', 'Virgin Islands, British', 'Virgin Islands, U.S.']\n",
      "Fist count:  24\n",
      "Second count:  222\n"
     ]
    }
   ],
   "source": [
    "#use rawCountriesRegions to get all countries and save ttl file\n",
    "\n",
    "\n",
    "\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "countriesNotFound=list()\n",
    "for index, row in countries.iterrows():  # iterate countriesregions\n",
    "    countryName = index.strip()\n",
    "    #print(countryName)\n",
    "\n",
    "    if countryName.lower() not in rawCountriesRegions.index.str.lower():\n",
    "        count1 += 1\n",
    "        countriesNotFound.append(countryName)\n",
    "\n",
    "    for elem in rawCountriesRegions.index:\n",
    "        elem = elem.strip()\n",
    "        if elem.lower() == countryName.lower():\n",
    "            count2 += 1\n",
    "        #else:\n",
    "        #if countryName not in countriesNotFound:\n",
    "        #countriesNotFound.append(countryName)\n",
    "\n",
    "\n",
    "    #if (countryName == countriesregions.index).any():\n",
    "\n",
    "    #if countryName not in countriesregions.index:\n",
    "    #print(countryName)\n",
    "#for elem in countriesregions.index:\n",
    "#        if elem == countryName:\n",
    "#            print(elem, '__aaaaaaa')\n",
    "#else:\n",
    "#    res = countriesregions.index.str.contains(countryName)\n",
    "#    print(countryName, '____ciao')\n",
    "\n",
    "#search inside countriesregions\n",
    "#for countriesregionsName in countriesregions.index:\n",
    "#if countryName in countriesregionsName:\n",
    "#print(countryName, ' ____')\n",
    "#print(' ')\n",
    "#else:\n",
    "#print(countryName)\n",
    "#print((countriesregions.index==countryName).any())\n",
    "print(countriesNotFound)\n",
    "print('Fist count: ', count1)\n",
    "print('Second count: ', count2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#countriesregions['countryID'] = setCountryID(countriesregions['Country'])\n",
    "\n",
    "#countriesregions.info()\n",
    "for index, row in rawCountriesRegions.iterrows():\n",
    "\n",
    "    for c in countries.index:\n",
    "        cName = c.strip()\n",
    "\n",
    "        if (countries.index == cName).any():\n",
    "            code = str(countries[countries.index == cName]['Alpha-2 code'][0]).lower()\n",
    "            #Country = URIRef(CNS[row['countryID']])\n",
    "            #g.add((Country, RDF.type, HLTB.Country))\n",
    "            #g.add((Country, HLTB['subClassOf'], Literal(row['Region'], datatype=XSD.string)))\n",
    "\n",
    "        g.add((HLTB[code], HLTB['subClassOf'], HLTB[row['Region']]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "countries = pd.read_csv(countriesPath, sep=',', index_col='Name', keep_default_na=False, na_values=['_'])\n",
    "countriesregions = pd.read_csv(countriesRegionsPath, sep=\",\", index_col='Country', names=['Country', 'Region'])\n",
    "\n",
    "rawCountriesRegions = pd.read_csv(rawCountriesRegionsPath, sep=\",\", index_col='name', names=['Country', 'Region'])\n",
    "\n",
    "g = createGraph()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "countriesregions['countryID'] = setCountryID(countriesregions['Country'])\n",
    "\n",
    "#countriesregions.info()\n",
    "for index, row in countriesregions.iterrows():\n",
    "\n",
    "    for c in countries.index:\n",
    "        cName = c.strip()\n",
    "\n",
    "        if (countries.index == cName).any():\n",
    "            code = str(countries[countries.index == cName]['Alpha-2 code'][0]).lower()\n",
    "            #Country = URIRef(CNS[row['countryID']])\n",
    "            #g.add((Country, RDF.type, HLTB.Country))\n",
    "            #g.add((Country, HLTB['subClassOf'], Literal(row['Region'], datatype=XSD.string)))\n",
    "\n",
    "        g.add((HLTB[code], HLTB['subClassOf'], HLTB[row['Region']]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count1 = 0\n",
    "count2 = 0\n",
    "countriesNotFound=list()\n",
    "for index, row in countries.iterrows():  # iterate countriesregions\n",
    "    countryName = index.strip()\n",
    "    #print(countryName)\n",
    "\n",
    "    if countryName.lower() not in countriesregions.index.str.lower():\n",
    "        count1 += 1\n",
    "        countriesNotFound.append(countryName)\n",
    "\n",
    "    for elem in countriesregions.index:\n",
    "        elem = elem.strip()\n",
    "        if elem.lower() == countryName.lower():\n",
    "            count2 += 1\n",
    "        #else:\n",
    "            #if countryName not in countriesNotFound:\n",
    "                #countriesNotFound.append(countryName)\n",
    "\n",
    "\n",
    "    #if (countryName == countriesregions.index).any():\n",
    "\n",
    "    #if countryName not in countriesregions.index:\n",
    "        #print(countryName)\n",
    "#for elem in countriesregions.index:\n",
    "#        if elem == countryName:\n",
    "#            print(elem, '__aaaaaaa')\n",
    "#else:\n",
    "#    res = countriesregions.index.str.contains(countryName)\n",
    "#    print(countryName, '____ciao')\n",
    "\n",
    "#search inside countriesregions\n",
    "#for countriesregionsName in countriesregions.index:\n",
    "#if countryName in countriesregionsName:\n",
    "#print(countryName, ' ____')\n",
    "#print(' ')\n",
    "#else:\n",
    "#print(countryName)\n",
    "#print((countriesregions.index==countryName).any())\n",
    "print(countriesNotFound)\n",
    "print('Fist count: ', count1)\n",
    "print('Second count: ', count2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for index, row in countries.iterrows():  # iterate countriesregions\n",
    "    countryName = index.strip()\n",
    "    #print(countryName)\n",
    "    if (countryName == countriesregions.index).any():\n",
    "        code = str(countries[countryName == countriesregions.index]['Alpha-2 code']).lower()\n",
    "        #print(code)\n",
    "        #print(code)\n",
    "        #Country = URIRef(CNS[row['countryID']])\n",
    "        #g.add((Country, RDF.type, HLTB.Country))\n",
    "        g.add((HLTB[code], HLTB['subClassOf'], HLTB[row['Region']]))\n",
    "    else:\n",
    "        print('ciao')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "countriesregions['countryID'] = setCountryID(countriesregions['Country'])\n",
    "#print(countries.index)\n",
    "#countriesregions.info()\n",
    "for index, row in countriesregions.iterrows():  # iterate countriesregions\n",
    "    countryName = row['Country'].strip()\n",
    "    #print(countryName)\n",
    "    if countryName in countries.index:\n",
    "        code = str(countries[countries.index == countryName]['Alpha-2 code'][0]).lower()\n",
    "        print(code)\n",
    "        #print(code)\n",
    "        #Country = URIRef(CNS[row['countryID']])\n",
    "        #g.add((Country, RDF.type, HLTB.Country))\n",
    "        g.add((HLTB[code], HLTB['subClassOf'], HLTB[row['Region']]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for c in countryName.split(','):  # iterate country\n",
    "    #print(index, c)\n",
    "    cName = c.strip()\n",
    "    #print(cName)\n",
    "    countryRegExToFind = '*[a-zA-Z].* .' + cName + '.* .*[a-zA-Z]'\n",
    "    #countryRegExToFind=countryRegExToFind.join(cName)\n",
    "    #countryRegExToFind=countryRegExToFind.join('.]')\n",
    "    #print(countryRegExToFind)\n",
    "    if cName in countries.index:\n",
    "        code = str(countries[countries.index == cName]['Alpha-2 code'][0]).lower()\n",
    "        print(code)\n",
    "        #print(code)\n",
    "        #Country = URIRef(CNS[row['countryID']])\n",
    "        #g.add((Country, RDF.type, HLTB.Country))\n",
    "        g.add((HLTB[code], HLTB['subClassOf'], HLTB[row['Region']]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "countriesregions['countryID'] = setCountryID(countriesregions['Country'])\n",
    "\n",
    "#countriesregions.info()\n",
    "for index, row in countriesregions.iterrows():\n",
    "\n",
    "    for c in str(row['Country']).split(','):\n",
    "        cName = c.strip()\n",
    "\n",
    "        if (countries.index == cName).any():\n",
    "            code = str(countries[countries.index == cName]['Alpha-2 code'][0]).lower()\n",
    "            Country = URIRef(CNS[row['countryID']])\n",
    "            g.add((Country, RDF.type, HLTB.Country))\n",
    "            g.add((Country, HLTB['subClassOf'], Literal(row['Region'], datatype=XSD.string)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the data in the Turtle format\n",
    "with open(countriesTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))\n",
    "\n",
    "print(\"Saved Regions TTL file.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Platform sales"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g = createGraph()\n",
    "platforms = pd.read_csv(platformsPath, sep=\",\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for id, row in platforms.iterrows():\n",
    "    if pd.notna(row[\"Platform\"]):\n",
    "\n",
    "        if pd.notna(row[\"Europe\"]):\n",
    "            PlatformSalesID = URIRef(HLTB[\"sales-\" + str(setPlatformID(row[\"Platform\"])) + \"___\" + \"eu\"])\n",
    "            g.add((PlatformSalesID, HLTB.Type, HLTB.Sale))\n",
    "            g.add((PlatformSalesID, HLTB[\"unitsSold\"], Literal(row[\"Europe\"], datatype=XSD.float)))\n",
    "            g.add((PlatformSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"eu\"])))\n",
    "            g.add((PlatformSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n",
    "\n",
    "        if pd.notna(row[\"Japan\"]):\n",
    "            PlatformSalesID = URIRef(HLTB[\"sales-\" + str(setPlatformID(row[\"Platform\"])) + \"___\" + \"jp\"])\n",
    "\n",
    "            g.add((PlatformSalesID, HLTB.Type, HLTB.Sale))\n",
    "            g.add((PlatformSalesID, HLTB[\"unitsSold\"], Literal(row[\"Japan\"], datatype=XSD.float)))\n",
    "            g.add((PlatformSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"jp\"])))\n",
    "            g.add((PlatformSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n",
    "\n",
    "        if pd.notna(row[\"North America\"]):\n",
    "            PlatformSalesID = URIRef(HLTB[\"sales-\" + str(setPlatformID(row[\"Platform\"])) + \"___\" + \"na\"])\n",
    "\n",
    "            g.add((PlatformSalesID, HLTB.Type, HLTB.Sale))\n",
    "            g.add((PlatformSalesID, HLTB[\"unitsSold\"], Literal(row[\"North America\"], datatype=XSD.float)))\n",
    "            g.add((PlatformSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"na\"])))\n",
    "            g.add((PlatformSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n",
    "\n",
    "        if pd.notna(row[\"Rest of World\"]):\n",
    "            PlatformSalesID = URIRef(HLTB[\"sales-\" + str(setPlatformID(row[\"Platform\"])) + \"___\" + \"other\"])\n",
    "\n",
    "            g.add((PlatformSalesID, HLTB.Type, HLTB.Sale))\n",
    "            g.add((PlatformSalesID, HLTB[\"unitsSold\"], Literal(row[\"Rest of World\"], datatype=XSD.float)))\n",
    "            g.add((PlatformSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"other\"])))\n",
    "            g.add((PlatformSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n",
    "\n",
    "        if pd.notna(row[\"Global\"]):\n",
    "            PlatformSalesID = URIRef(HLTB[\"sales-\" + str(setPlatformID(row[\"Platform\"])) + \"___\" + \"global\"])\n",
    "            g.add((PlatformSalesID, HLTB.Type, HLTB.Sale))\n",
    "            g.add((PlatformSalesID, HLTB[\"unitsSold\"], Literal(row[\"Global\"], datatype=XSD.float)))\n",
    "            g.add((PlatformSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"global\"])))\n",
    "            g.add((PlatformSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data in the Turtle format\n",
    "with open(platformsSalesTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))\n",
    "\n",
    "print(\"Saved platforms Sales TTL file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
