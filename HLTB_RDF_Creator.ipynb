{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "KTj_MOZOmzfn"
      },
      "source": [
        "# HowLongToBeat RDF Creator\n",
        "\n",
        "We load the generated CSV files and we serialize all the data into ***turtle format  (TTL)*** relying on ***RDFLib*** Python library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "GPtFAXlZmzfp"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We import all the necessary libraries and we set the paths to the input/output files. In particular, we create a TTL file for each type of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JXZwhL0Bmzfp"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Load the required libraries\n",
        "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
        "\n",
        "# RDFLib knows about some namespaces, like XSD\n",
        "from rdflib.namespace import XSD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "LrSYA9LWmzfq"
      },
      "outputs": [],
      "source": [
        "absPath = str(Path(os.path.abspath(os.getcwd())).absolute())\n",
        "datasetsPath = os.path.join(absPath, \"cleaned_datasets\")\n",
        "rawdatasetsPath = os.path.join(absPath, \"raw_datasets\")\n",
        "rdfPath = os.path.join(absPath, \"rdf\")\n",
        "\n",
        "# Create dataset directory if not exists\n",
        "if not os.path.exists(datasetsPath):\n",
        "    os.mkdir(datasetsPath)\n",
        "\n",
        "# Create RDF directory if not exists\n",
        "if not os.path.exists(rdfPath):\n",
        "    os.mkdir(rdfPath)\n",
        "\n",
        "# Setup datasets paths\n",
        "gamesPath = os.path.join(datasetsPath, \"games_cleaned.csv\")\n",
        "vgchartzPath = os.path.join(datasetsPath, \"vgchartz_cleaned.csv\")\n",
        "indiegamesdevelopersPath = os.path.join(datasetsPath, \"indiegamesdevelopers_cleaned_seriesExplode.csv\")\n",
        "platformsPath = os.path.join(datasetsPath,\"platforms.csv\" )\n",
        "videoGameDevelopersPath = os.path.join(datasetsPath, \"videogamesdevelopers_cleaned_seriesexplode.csv\" )\n",
        "completionTimePath = os.path.join(datasetsPath, \"completion_time.csv\")\n",
        "\n",
        "# Setup raw datasets\n",
        "rawVGChartsPath = os.path.join(rawdatasetsPath, \"vgchartz-7_7_2020.csv\")\n",
        "rawGamesPath = os.path.join(rawdatasetsPath, \"games.csv\")\n",
        "\n",
        "\n",
        "# Countries-Regions path\n",
        "countriesRegionsPath = os.path.join(datasetsPath, \"countries-regions.csv\")\n",
        "countriesPath = os.path.join(absPath, \"wikipedia-iso-country-codes.csv\")\n",
        "\n",
        "\n",
        "# Setup Turtle paths\n",
        "genresTTLPath = os.path.join(rdfPath, \"genres.ttl\")\n",
        "gamesTTLPath = os.path.join(rdfPath, \"games.ttl\")\n",
        "companyTTLPath = os.path.join(rdfPath, \"company.ttl\")\n",
        "platformsTTLPath = os.path.join(rdfPath, \"platforms.ttl\")\n",
        "videoGameDevelopersTTLPath = os.path.join(rdfPath, \"videoGameDevelopers.ttl\")\n",
        "statsTTLPath = os.path.join(rdfPath, \"stats.ttl\")\n",
        "gameSalesTTLPath = os.path.join(rdfPath, \"gameSales.ttl\")\n",
        "regionsTTLPath = os.path.join(rdfPath, \"regions.ttl\")\n",
        "countriesregionsTTLPath = os.path.join(rdfPath, \"countriesregions.ttl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8rWK8cFyruCz"
      },
      "outputs": [],
      "source": [
        "# Country Ontology\n",
        "CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
        "\n",
        "# HLTB Ontology\n",
        "HLTB = Namespace(\"http://www.semanticweb.org/enrico/ontologies/2022/10/HLTB-db2unipd#\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "J3Pw_mt1ruCz"
      },
      "outputs": [],
      "source": [
        "def createGraph():\n",
        "    # Create the graph\n",
        "    g = Graph()\n",
        "\n",
        "    # Bind the namespaces to a prefix for more readable output\n",
        "    g.bind(\"xsd\", XSD)\n",
        "    g.bind(\"countries\", CNS)\n",
        "    g.bind(\"hltb\", HLTB)\n",
        "\n",
        "    return g\n",
        "\n",
        "#create game URI\n",
        "def createGameID(title):\n",
        "    # Replace all special chars with \"-\"\n",
        "    gameID = \"\"\n",
        "    for char in title:\n",
        "        if char.isalnum():\n",
        "            gameID += char\n",
        "        elif len(gameID) > 0 and gameID[-1] != '-':\n",
        "            gameID += '-'\n",
        "    if len(gameID) > 0 and gameID[-1] == '-':\n",
        "        gameID = gameID[:-1]\n",
        "    #print(gameID.lower())\n",
        "    return gameID.lower()\n",
        "\n",
        "#Create genre URI\n",
        "def setGenreID(genre): ##first half is the original genres, second half are processed and lowercase\n",
        "    genre = str(genre).replace(\"/\",\", \").replace(\"nan\",\"\")\n",
        "    genre=genre.split(\", \")\n",
        "    list=[]\n",
        "    for i in range(len(genre)):\n",
        "        list.append([])\n",
        "        list[i].append(genre[i])\n",
        "        list[i].append(genre[i].lower().replace(\"'\",\"\").replace(\" \", \"-\"))\n",
        "    return(list)\n",
        "\n",
        "\n",
        "def setPlatformID(platform):\n",
        "    return(platform.lower().replace(\" \", \"-\").replace(\"/\",\"-\").replace(\"&\",\"-\"))\n",
        "\n",
        "def setDeveloperID(developer):\n",
        "    return developer.replace(' ', '-', regex=True).str.lower()\n",
        "\n",
        "def setCountryID(country):\n",
        "    return country.replace(' ', '-', regex=True).str.lower()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "iXu4WZdPruC0"
      },
      "source": [
        "## Serialization\n",
        "\n",
        "We serialize the data according to the following workflow:\n",
        "\n",
        "1. Load the CSV file and iterate through it\n",
        "2. Create a unique ID by ourself based on the name of the class.\n",
        "3. Add the node to the graph using the unique ID.\n",
        "4. Add all the data properties.\n",
        "5. Add all the object properties.\n",
        "6. Serialize the data and save them into a TTL file.\n",
        "\n",
        "### Games\n",
        "\n",
        "Now serializing the Game class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "U8LlSWkZruC0"
      },
      "outputs": [],
      "source": [
        "# Create Graph\n",
        "g = createGraph()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2FNvk7RTruC0"
      },
      "outputs": [],
      "source": [
        "# Load the CSV files in memory\n",
        "games = pd.read_csv(gamesPath, sep=\",\",index_col=\"title\")\n",
        "vgchartz = pd.read_csv(vgchartzPath, sep=\",\")\n",
        "platforms = pd.read_csv(platformsPath,sep=\",\")\n",
        "\n",
        "merged = pd.merge(vgchartz, platforms, left_on='console', right_on='Acronym', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "I07K-83_ruC0"
      },
      "outputs": [],
      "source": [
        "# Iterate over the games\n",
        "for title, row in games.iterrows():\n",
        "    # Create gameID from its title\n",
        "    gameID = createGameID(title)\n",
        "\n",
        "    # Create the node to add to the Graph\n",
        "    Game = URIRef(HLTB[gameID])\n",
        "\n",
        "    # Add triples using store's add() method.\n",
        "    g.add((Game, RDF.type, HLTB.Game))\n",
        "\n",
        "    # Add the title of the game\n",
        "    g.add((Game, HLTB[\"officialName\"], Literal(title, datatype=XSD.string)))\n",
        "\n",
        "    # Add multiplayer focus\n",
        "    g.add((Game, HLTB[\"multiplayerFocus\"], Literal(pd.notnull(row[\"coop\"]) or pd.notnull(row[\"versus\"]), datatype=XSD.boolean)))\n",
        "\n",
        "    #Add hltb id\n",
        "    g.add((Game, HLTB[\"id\"], Literal(row[\"id\"], datatype=XSD.int)))\n",
        "\n",
        "    #Add hasGenre object property\n",
        "    for iterator in setGenreID(row[\"genres\"]):\n",
        "        if(pd.notnull(iterator[1]) and iterator[1] != ''):\n",
        "                g.add((Game, HLTB[\"hasGenre\"], URIRef(HLTB[iterator[1]])))\n",
        "\n",
        "    #Add platform availability\n",
        "    if pd.notna(row[\"platforms\"]):\n",
        "        for platform in row[\"platforms\"].split(\", \"):\n",
        "            g.add((Game, HLTB[\"releasedOn\"], URIRef(HLTB[setPlatformID(platform)])))\n",
        "\n",
        "             #Add Stats object property\n",
        "            g.add((Game, HLTB[\"hasStats\"], URIRef(HLTB[\"stats-\"+str(createGameID(title)) + \"___\" + str(setPlatformID(platform))])))\n",
        "\n",
        "            # Add Sales object property\n",
        "            game = merged.loc[(merged['title'] == title) & (merged['Platform'] == platform)]\n",
        "            if not game.empty:\n",
        "                if (pd.notna(game[\"pal_sales\"].iloc[0])):\n",
        "                    GameSalesID=URIRef(HLTB[\"sales-\"+str(createGameID(title))+\"___\"+str(setPlatformID(platform))+\"___\"+\"eu\"])\n",
        "                    g.add((Game, HLTB[\"sold\"], GameSalesID))\n",
        "                if (pd.notna(game[\"na_sales\"].iloc[0])):\n",
        "                    GameSalesID=URIRef(HLTB[\"sales-\"+str(createGameID(title))+\"___\"+str(setPlatformID(platform))+\"___\"+\"na\"])\n",
        "                    g.add((Game, HLTB[\"sold\"], GameSalesID))\n",
        "                if (pd.notna(game[\"jp_sales\"].iloc[0])):\n",
        "                    GameSalesID=URIRef(HLTB[\"sales-\"+str(createGameID(title))+\"___\"+str(setPlatformID(platform))+\"___\"+\"jp\"])\n",
        "                    g.add((Game, HLTB[\"sold\"], GameSalesID))\n",
        "                if (pd.notna(game[\"other_sales\"].iloc[0])):\n",
        "                    GameSalesID=URIRef(HLTB[\"sales-\"+str(createGameID(title))+\"___\"+str(setPlatformID(platform))+\"___\"+\"other\"])\n",
        "                    g.add((Game, HLTB[\"sold\"], GameSalesID))\n",
        "                if (pd.notna(game[\"total_shipped\"].iloc[0])):\n",
        "                    GameSalesID=URIRef(HLTB[\"sales-\"+str(createGameID(title))+\"___\"+str(setPlatformID(platform))+\"___\"+\"global\"])\n",
        "                    g.add((Game, HLTB[\"sold\"], GameSalesID))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "F3LkfeExruC1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "A6W6ailyruC1",
        "outputId": "907e170f-07d8-403a-db97-375eb8cfe650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved games TTL file.\n"
          ]
        }
      ],
      "source": [
        "# Save the data in the Turtle format\n",
        "with open(gamesTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
        "    fp.write(g.serialize(format=\"turtle\"))\n",
        "\n",
        "print(\"Saved games TTL file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "4w_Ex0ozruC1"
      },
      "source": [
        "### Genre\n",
        "\n",
        "Now serializing the Genre class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Iq5up3yxruC1"
      },
      "outputs": [],
      "source": [
        "# Create Graph\n",
        "g = createGraph()\n",
        "# Load the CSV files in memory\n",
        "genres = pd.read_csv(gamesPath, sep=\",\", index_col=\"genres\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JIm43S8ZruC1"
      },
      "outputs": [],
      "source": [
        "for genre, row in genres.iterrows():\n",
        "    allGenres = setGenreID(genre)\n",
        "    for iterator in allGenres:\n",
        "        if not (iterator[0] == \" \" or iterator[0] == \"\"):\n",
        "            Genre = URIRef(HLTB[iterator[1]])\n",
        "            #Add triples using store's add() method.\n",
        "            g.add((Genre, RDF.type, HLTB.Genre))\n",
        "            # Add the name of the genre\n",
        "            g.add((Genre, HLTB[\"name\"], Literal(iterator[0], datatype=XSD.string)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Y5aZLe9bruC2"
      },
      "outputs": [],
      "source": [
        "# Save genre data in the Turtle format\n",
        "with open(genresTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
        "    fp.write(g.serialize(format=\"turtle\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gPsY16D1ruC2"
      },
      "source": [
        "### Platforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yX0sNK0IruC2"
      },
      "outputs": [],
      "source": [
        "# Create Graph\n",
        "g = createGraph()\n",
        "# Load the CSV files in memory\n",
        "platforms = pd.read_csv(platformsPath, sep=\",\", index_col=\"Platform\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13l-L1GCruC2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FPGiytrTruC2"
      },
      "outputs": [],
      "source": [
        "for platform, row in platforms.iterrows():\n",
        "    Platform = URIRef(HLTB[setPlatformID(platform)])\n",
        "    #Add triples using store's add() method.\n",
        "    g.add((Platform, RDF.type, HLTB.Platform))\n",
        "    # Add the name of the genre\n",
        "    g.add((Platform, HLTB[\"name\"], Literal(platform, datatype=XSD.string)))\n",
        "\n",
        "    #Add popularity if platform is popular\n",
        "    if(row[\"Popular\"] == True):\n",
        "        g.add((Platform, HLTB[\"popular\"], Literal(True, datatype=XSD.boolean)))\n",
        "\n",
        "    #Add release date if present\n",
        "    if pd.notna(row[\"Release date\"]):\n",
        "        time = datetime.combine(datetime.strptime(row[\"Release date\"], '%Y-%M-%d'), datetime.min.time())\n",
        "        g.add((Platform, HLTB[\"releaseDate\"], Literal(time,datatype=XSD.dateTime)))\n",
        "\n",
        "    #Add CPU information\n",
        "    if pd.notna(row[\"CPU\"]):\n",
        "        g.add((Platform, HLTB[\"cpu\"], Literal(row[\"CPU\"],datatype=XSD.string)))\n",
        "\n",
        "    #Add CPU bit information\n",
        "    if pd.notna(row[\"\\\"Bits\\\"\"]):\n",
        "        bits = row[\"\\\"Bits\\\"\"].split(\"-\")[0]\n",
        "        g.add((Platform, HLTB[\"bits\"], Literal(bits,datatype=XSD.int)))\n",
        "\n",
        "    #Add acronym information\n",
        "    if pd.notna(row[\"Acronym\"]):\n",
        "        g.add((Platform, HLTB[\"acronym\"], Literal(row[\"Acronym\"],datatype=XSD.string)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9Dxz7tEOruC2"
      },
      "outputs": [],
      "source": [
        "# Save data in the Turtle format\n",
        "with open(platformsTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
        "    fp.write(g.serialize(format=\"turtle\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "PORFYtu0ruC2"
      },
      "source": [
        "### Stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "I68ymABrruC2"
      },
      "outputs": [],
      "source": [
        "g = createGraph()\n",
        "games = pd.read_csv(gamesPath, sep=\",\")\n",
        "vgchartz = pd.read_csv(vgchartzPath, sep=\",\")\n",
        "completionTime = pd.read_csv(completionTimePath, sep=\",\")\n",
        "platforms = pd.read_csv(platformsPath,sep=\",\")\n",
        "\n",
        "merged_temp = pd.merge(games, completionTime, left_on=  'id',\n",
        "                  right_on= 'gameID')\n",
        "merged_temp1 = pd.merge(vgchartz, platforms, left_on='console', right_on='Acronym', how='left')\n",
        "merged = pd.merge(merged_temp, merged_temp1, left_on = [\"title\",\"platform\"], right_on=[\"title\",\"Platform\"], how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ttf7puJwruC2"
      },
      "outputs": [],
      "source": [
        "for id, row in merged.iterrows():\n",
        "    if(pd.notna(row[\"title\"]) and pd.notna(row[\"platform\"])):\n",
        "        StatsID = URIRef(HLTB[createGameID(\"stats-\"+str(createGameID(row[\"title\"]))) + \"___\" + str(setPlatformID(row[\"platform\"]))])\n",
        "\n",
        "        #Adding node type\n",
        "        g.add((StatsID, HLTB.Type, HLTB.Stats))\n",
        "\n",
        "        #Add Time information\n",
        "        g.add((StatsID, HLTB[\"polledTime\"], Literal(row[\"count_comp\"],datatype=XSD.int)))\n",
        "        g.add((StatsID, HLTB[\"mainTime\"],Literal(row[\"comp_main\"], datatype=XSD.int)))\n",
        "        g.add((StatsID, HLTB[\"mainPlusTime\"],Literal(row[\"comp_plus\"], datatype=XSD.int)))\n",
        "        g.add((StatsID, HLTB[\"completionistTime\"],Literal(row[\"comp_100\"], datatype=XSD.int)))\n",
        "        g.add((StatsID, HLTB[\"slowestTime\"],Literal(row[\"comp_low\"], datatype=XSD.int)))\n",
        "        g.add((StatsID, HLTB[\"fastestTime\"],Literal(row[\"comp_high\"], datatype=XSD.int)))\n",
        "\n",
        "        #Add remaining stats\n",
        "        if pd.notna(row[\"critic_score\"]):\n",
        "            g.add((StatsID, HLTB[\"criticScore\"], Literal(row[\"critic_score\"],datatype=XSD.float)))\n",
        "        if pd.notna(row[\"user_score\"]):\n",
        "            g.add((StatsID, HLTB[\"userScore\"], Literal(row[\"user_score\"], datatype=XSD.float)))\n",
        "\n",
        "        if pd.notna(row[\"release_date\"]):\n",
        "            time = datetime.combine(datetime.strptime(row[\"release_date\"], '%Y-%M-%d'), datetime.min.time())\n",
        "            g.add((StatsID, HLTB[\"releaseDate\"], Literal(time,datatype=XSD.dateTime)))\n",
        "\n",
        "        #Add \"onPlatform\" object property\n",
        "        g.add((StatsID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"platform\"])])))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JupeUh_YruC2",
        "outputId": "a3e1cf27-ace3-404c-ca9b-5a6e385c5b55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved company TTL file.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Save the data in the Turtle format\n",
        "with open(statsTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
        "    fp.write(g.serialize(format=\"turtle\"))\n",
        "\n",
        "print(\"Saved company TTL file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "zfNp0kldruC3"
      },
      "source": [
        "### Game sales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VoXuJ8QPruC3"
      },
      "outputs": [],
      "source": [
        "g = createGraph()\n",
        "vgchartz = pd.read_csv(vgchartzPath, sep=\",\")\n",
        "platforms = pd.read_csv(platformsPath,sep=\",\")\n",
        "\n",
        "merged = pd.merge(vgchartz, platforms, left_on='console', right_on='Acronym', how='left')\n",
        "merged.to_csv(\"prova.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MVFwUsK1ruC3"
      },
      "outputs": [],
      "source": [
        "for id, row in merged.iterrows():\n",
        "    if(pd.notna(row[\"Platform\"])):\n",
        "\n",
        "        if (pd.notna(row[\"pal_sales\"])):\n",
        "            GameSalesID=URIRef(HLTB[\"sales-\"+str(createGameID(row[\"title\"]))+\"___\"+str(setPlatformID(row[\"Platform\"]))+\"___\"+\"eu\"])\n",
        "            g.add((GameSalesID, HLTB.Type, HLTB.Sales))\n",
        "            g.add((GameSalesID, HLTB[\"unitsSold\"], Literal(row[\"pal_sales\"], datatype=XSD.float)))\n",
        "            g.add((GameSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"eu\"])))\n",
        "            g.add((GameSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n",
        "\n",
        "        if (pd.notna(row[\"jp_sales\"])):\n",
        "            GameSalesID=URIRef(HLTB[\"sales-\"+str(createGameID(row[\"title\"]))+\"___\"+str(setPlatformID(row[\"Platform\"]))+\"___\"+\"jp\"])\n",
        "            g.add((GameSalesID, HLTB.Type, HLTB.Sales))\n",
        "            g.add((GameSalesID, HLTB[\"unitsSold\"], Literal(row[\"jp_sales\"], datatype=XSD.float)))\n",
        "            g.add((GameSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"jp\"])))\n",
        "            g.add((GameSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n",
        "\n",
        "        if (pd.notna(row[\"na_sales\"])):\n",
        "            GameSalesID=URIRef(HLTB[\"sales-\"+str(createGameID(row[\"title\"]))+\"___\"+str(setPlatformID(row[\"Platform\"]))+\"___\"+\"na\"])\n",
        "            g.add((GameSalesID, HLTB.Type, HLTB.Sales))\n",
        "            g.add((GameSalesID, HLTB[\"unitsSold\"], Literal(row[\"na_sales\"], datatype=XSD.float)))\n",
        "            g.add((GameSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"na\"])))\n",
        "            g.add((GameSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n",
        "\n",
        "        if (pd.notna(row[\"other_sales\"])):\n",
        "            GameSalesID=URIRef(HLTB[\"sales-\"+str(createGameID(row[\"title\"]))+\"___\"+str(setPlatformID(row[\"Platform\"]))+\"___\"+\"other\"])\n",
        "            g.add((GameSalesID, HLTB.Type, HLTB.Sales))\n",
        "            g.add((GameSalesID, HLTB[\"unitsSold\"], Literal(row[\"other_sales\"], datatype=XSD.float)))\n",
        "            g.add((GameSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"other\"])))\n",
        "            g.add((GameSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n",
        "\n",
        "        if (pd.notna(row[\"total_shipped\"])):\n",
        "            GameSalesID=URIRef(HLTB[\"sales-\"+str(createGameID(row[\"title\"]))+\"___\"+str(setPlatformID(row[\"Platform\"]))+\"___\"+\"global\"])\n",
        "            g.add((GameSalesID, HLTB.Type, HLTB.Sales))\n",
        "            g.add((GameSalesID, HLTB[\"unitsSold\"], Literal(row[\"total_shipped\"], datatype=XSD.float)))\n",
        "            g.add((GameSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"global\"])))\n",
        "            g.add((GameSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QnD2FYA-ruC3",
        "outputId": "f8c273c3-e30c-4310-b450-f55ca63c3ca4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved company TTL file.\n"
          ]
        }
      ],
      "source": [
        "# Save the data in the Turtle format\n",
        "with open(gameSalesTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
        "    fp.write(g.serialize(format=\"turtle\"))\n",
        "\n",
        "print(\"Saved company TTL file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVPWWFGcnDbO"
      },
      "source": [
        "### Company"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-Z3-vXofruC3"
      },
      "outputs": [],
      "source": [
        "# Create Graph\n",
        "g = createGraph()\n",
        "# Load the CSV files in memory\n",
        "\n",
        "indiegamesdevelopers = pd.read_csv(indiegamesdevelopersPath, sep=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3WglgCByruC3"
      },
      "outputs": [],
      "source": [
        "#replace space with -, lower case\n",
        "indiegamesdevelopers['companyID'] = setDeveloperID(indiegamesdevelopers['Developer'])\n",
        "\n",
        "\n",
        "#indiegamesdevelopers.info()\n",
        "for index, row in indiegamesdevelopers.iterrows():\n",
        "  # Create the node to add to the Graph\n",
        "  Company = URIRef(HLTB[row['companyID']])\n",
        "\n",
        "  # Add triples using store's add() method.\n",
        "  g.add((Company, RDF.type, HLTB.Company))\n",
        "\n",
        "  # Add the Company\n",
        "  g.add((Company, HLTB[\"indieDeveloper\"], Literal(pd.notnull(row['Developer']), datatype=XSD.boolean)))\n",
        "  g.add((Company, HLTB['officialName'], Literal(row['Developer'], datatype=XSD.string)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "06I0sgugruC3"
      },
      "source": [
        "### video-game-evelopers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Qx1fjKBUruC4"
      },
      "outputs": [],
      "source": [
        "videoGameDevelopers = pd.read_csv(videoGameDevelopersPath, sep=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fSebeiW4ruC4"
      },
      "outputs": [],
      "source": [
        "#replace space with -, lower case\n",
        "videoGameDevelopers['companyID'] = setDeveloperID(videoGameDevelopers['Developer'])\n",
        "\n",
        "\n",
        "#indiegamesdevelopers.info()\n",
        "for index, row in videoGameDevelopers.iterrows():\n",
        "  # Create the node to add to the Graph\n",
        "  Company = URIRef(HLTB[row['companyID']])\n",
        "\n",
        "  # Add triples using store's add() method.\n",
        "  g.add((Company, RDF.type, HLTB.Company))\n",
        "\n",
        "  # Add the Company\n",
        "  g.add((Company, HLTB['officialName'], Literal(row['Developer'], datatype=XSD.string)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dbKlh6J9ozNy",
        "outputId": "9c3659ad-c271-402f-863c-2aea7d2d9890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved company TTL file.\n"
          ]
        }
      ],
      "source": [
        "# Save the data in the Turtle format\n",
        "with open(companyTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
        "    fp.write(g.serialize(format=\"turtle\"))\n",
        "\n",
        "print(\"Saved company TTL file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Region"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Graph\n",
        "g = createGraph()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Graph identifier=N4b0823f9176749a7a4e8c6962048d4a1 (<class 'rdflib.graph.Graph'>)>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Create the node to add to the Graph\n",
        "Region = URIRef(HLTB['eu'])\n",
        "g.add((Region, RDF.type, HLTB.Region))\n",
        "g.add((Region, HLTB[{'eu', 'jp', 'na', 'other'}], Literal('eu', datatype=XSD.string)))\n",
        "\n",
        "Region = URIRef(HLTB['jp'])\n",
        "g.add((Region, RDF.type, HLTB.Region))\n",
        "g.add((Region, HLTB[{'eu', 'jp', 'na', 'other'}], Literal('jp', datatype=XSD.string)))\n",
        "\n",
        "\n",
        "Region = URIRef(HLTB['na'])\n",
        "g.add((Region, RDF.type, HLTB.Region))\n",
        "g.add((Region, HLTB[{'eu', 'jp', 'na', 'other'}], Literal('na', datatype=XSD.string)))\n",
        "\n",
        "\n",
        "Region = URIRef(HLTB['other'])\n",
        "g.add((Region, RDF.type, HLTB.Region))\n",
        "g.add((Region, HLTB[{'eu', 'jp', 'na', 'other'}], Literal('other', datatype=XSD.string)))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved Regions TTL file.\n"
          ]
        }
      ],
      "source": [
        "# Save the data in the Turtle format\n",
        "with open(regionsTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
        "    fp.write(g.serialize(format=\"turtle\"))\n",
        "\n",
        "print(\"Saved Regions TTL file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Country"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the CSV files in memory\n",
        "countries = pd.read_csv(countriesPath, sep=',', index_col='Name', keep_default_na=False, na_values=['_'])\n",
        "countriesregions = pd.read_csv(countriesRegionsPath, sep=\",\", names=['Country' , 'Region'])\n",
        "\n",
        "g = createGraph()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "countriesregions['countryID'] = setCountryID(countriesregions['Country'])\n",
        "\n",
        "#countriesregions.info()\n",
        "for index, row in countriesregions.iterrows():\n",
        "  Region = URIRef(HLTB[row['countryID']])\n",
        "  \n",
        "  for c in str(row['Country']).split(','):\n",
        "    cName = row['Country'].strip()\n",
        "    if((countries.index == cName).any() == True):\n",
        "      code = str(countries[countries.index == cName]['Alpha-2 code'][0]).lower()\n",
        "      Country = URIRef(CNS[code])\n",
        "\n",
        "      if row['Region']=='eu':\n",
        "        g.add((Region, HLTB['subClassOf'], Literal(Country, datatype=XSD.string)))\n",
        "\n",
        "      elif row['Region']=='jp':\n",
        "        g.add((Region, HLTB['subClassOf'], Literal(Country, datatype=XSD.string)))\n",
        "        \n",
        "      elif row['Region']=='na':\n",
        "        g.add((Region, HLTB['subClassOf'], Literal(Country, datatype=XSD.string)))\n",
        "        \n",
        "      else:\n",
        "        g.add((Region, HLTB['subClassOf'], Literal(Country, datatype=XSD.string)))\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved Regions TTL file.\n"
          ]
        }
      ],
      "source": [
        "# Save the data in the Turtle format\n",
        "with open(countriesregionsTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
        "    fp.write(g.serialize(format=\"turtle\"))\n",
        "\n",
        "print(\"Saved Regions TTL file.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
