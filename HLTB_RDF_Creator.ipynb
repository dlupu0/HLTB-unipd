{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTj_MOZOmzfn"
   },
   "source": [
    "# HowLongToBeat RDF Creator\n",
    "\n",
    "We load the generated CSV files and we serialize all the data into ***turtle format  (TTL)*** relying on ***RDFLib*** Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPtFAXlZmzfp"
   },
   "source": [
    "## Setup\n",
    "\n",
    "We import all the necessary libraries and we set the paths to the input/output files. In particular, we create a TTL file for each type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "JXZwhL0Bmzfp"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace, RDFS\n",
    "# RDFLib knows about some namespaces, like XSD\n",
    "from rdflib.namespace import XSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "absPath = str(Path(os.path.abspath(os.getcwd())).absolute())\n",
    "datasetsPath = os.path.join(absPath, \"cleaned_datasets\")\n",
    "rawdatasetsPath = os.path.join(absPath, \"raw_datasets\")\n",
    "rdfPath = os.path.join(absPath, \"rdf\")\n",
    "\n",
    "# Create dataset directory if not exists\n",
    "if not os.path.exists(datasetsPath):\n",
    "    os.mkdir(datasetsPath)\n",
    "\n",
    "# Create RDF directory if not exists\n",
    "if not os.path.exists(rdfPath):\n",
    "    os.mkdir(rdfPath)\n",
    "\n",
    "# Setup datasets paths\n",
    "gamesPath = os.path.join(datasetsPath, \"games_cleaned.csv\")\n",
    "vgchartzPath = os.path.join(datasetsPath, \"vgchartz_cleaned.csv\")\n",
    "indiegamesdevelopersPath = os.path.join(datasetsPath, \"indiegamesdevelopers_cleaned_seriesExplode.csv\")\n",
    "platformsPath = os.path.join(datasetsPath, \"platforms.csv\")\n",
    "videoGameDevelopersPath = os.path.join(datasetsPath, \"videogamesdevelopers_cleaned_seriesexplode.csv\")\n",
    "completionTimePath = os.path.join(datasetsPath, \"completion_time.csv\")\n",
    "\n",
    "# Setup raw datasets\n",
    "rawVGChartsPath = os.path.join(rawdatasetsPath, \"vgchartz-7_7_2020.csv\")\n",
    "rawGamesPath = os.path.join(rawdatasetsPath, \"games.csv\")\n",
    "rawCountriesRegionsPath = os.path.join(rawdatasetsPath, 'countries-regions.csv')\n",
    "\n",
    "# Setup Turtle paths\n",
    "genresTTLPath = os.path.join(rdfPath, \"genres.ttl\")\n",
    "gamesTTLPath = os.path.join(rdfPath, \"games.ttl\")\n",
    "companiesTTLPath = os.path.join(rdfPath, \"companies.ttl\")\n",
    "platformsTTLPath = os.path.join(rdfPath, \"platforms.ttl\")\n",
    "platformsSalesTTLPath = os.path.join(rdfPath, \"platformsSales.ttl\")\n",
    "\n",
    "videoGameDevelopersTTLPath = os.path.join(rdfPath, \"videoGameDevelopers.ttl\")\n",
    "statsTTLPath = os.path.join(rdfPath, \"stats.ttl\")\n",
    "gameSalesTTLPath = os.path.join(rdfPath, \"gameSales.ttl\")\n",
    "regionsTTLPath = os.path.join(rdfPath, \"regions.ttl\")\n",
    "countriesTTLPath = os.path.join(rdfPath, \"countries.ttl\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# Country Ontology\n",
    "CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "\n",
    "# HLTB Ontology\n",
    "HLTB = Namespace(\"http://www.dei.unipd.it/database2/HLTB-db2unipd#\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def createGraph():\n",
    "    # Create the graph\n",
    "    g = Graph()\n",
    "\n",
    "    # Bind the namespaces to a prefix for more readable output\n",
    "    g.bind(\"xsd\", XSD)\n",
    "    g.bind(\"countries\", CNS)\n",
    "    g.bind(\"hltb\", HLTB)\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "#create game URI\n",
    "def createGameID(title):\n",
    "    # Replace all special chars with \"-\"\n",
    "    gameID = \"\"\n",
    "    for char in title:\n",
    "        if char.isalnum():\n",
    "            gameID += char\n",
    "        elif len(gameID) > 0 and gameID[-1] != '-':\n",
    "            gameID += '-'\n",
    "    return gameID.strip('-').lower()\n",
    "\n",
    "\n",
    "#Create genre URI\n",
    "def setGenreID(genre):\n",
    "    #first half is the original genres, second half are processed and lowercase\n",
    "    genre = str(genre).replace(\"/\", \", \").replace(\"nan\", \"\")\n",
    "    genre = genre.split(\", \")\n",
    "    list = []\n",
    "    for i in range(len(genre)):\n",
    "        list.append([])\n",
    "        list[i].append(genre[i])\n",
    "        genreID = \"\"\n",
    "        for t in genre[i]:\n",
    "            if t.isalnum():\n",
    "                genreID = genreID + t\n",
    "            else:\n",
    "                genreID = genreID + \"-\"\n",
    "\n",
    "        list[i].append(genreID.lower().strip(\"-\"))\n",
    "    return list\n",
    "\n",
    "\n",
    "def setPlatformID(platform):\n",
    "    platformID = ''\n",
    "    for i in platform:\n",
    "        if i.isalnum():\n",
    "            platformID = platformID + i\n",
    "        elif len(platformID) > 0 and platformID[-1] != '-':\n",
    "            platformID = platformID + \"-\"\n",
    "    return platformID.strip('-').lower()\n",
    "\n",
    "\n",
    "def setCompanyID(company):\n",
    "    companyID = ''\n",
    "    for i in company:\n",
    "        if i.isalnum():\n",
    "            companyID = companyID + i\n",
    "        elif len(companyID) > 0 and companyID[-1] != '-':\n",
    "            companyID = companyID + \"-\"\n",
    "    return companyID.strip('-').lower()\n",
    "\n",
    "\n",
    "def getCountry2Digits(country):\n",
    "    rawCountriesRegions = pd.read_csv(rawCountriesRegionsPath, sep=\",\", index_col='name')\n",
    "    if 'namibia' == country.lower():\n",
    "        return 'na'\n",
    "    if 'united states' == country.lower():\n",
    "        return 'us'\n",
    "    if 'united kingdom' in country.lower() or 'england' in country.lower():\n",
    "        return 'gb'\n",
    "    if country in rawCountriesRegions.index:\n",
    "        return rawCountriesRegions[rawCountriesRegions.index == country]['alpha-2'].values[0].lower()\n",
    "    elif country.lower() in rawCountriesRegions['alpha-2']:\n",
    "        return country\n",
    "    return ''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Serialization\n",
    "\n",
    "We serialize the data according to the following workflow:\n",
    "\n",
    "1. Load the CSV file and iterate through it, merging them if needed.\n",
    "2. Create a unique ID by ourselves based on the name of the class.\n",
    "3. Add the node to the graph using the unique ID.\n",
    "4. Add all the data properties.\n",
    "5. Add all the object properties.\n",
    "6. Serialize the data and save them into a TTL file.\n",
    "\n",
    "## Games\n",
    "\n",
    "Now serializing the Game class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "games = pd.read_csv(gamesPath, sep=\",\", index_col=\"title\")\n",
    "vgchartz = pd.read_csv(vgchartzPath, sep=\",\")\n",
    "platforms = pd.read_csv(platformsPath, sep=\",\")\n",
    "\n",
    "merged = pd.merge(vgchartz, platforms, left_on='console', right_on='Acronym', how='left')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# Iterate over the games\n",
    "for title, row in games.iterrows():\n",
    "    # Create gameID from its title\n",
    "    gameID = createGameID(title)\n",
    "\n",
    "    # Create the node to add to the Graph\n",
    "    Game = URIRef(HLTB[gameID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Game, RDF.type, HLTB.Game))\n",
    "\n",
    "    # Add the title of the game\n",
    "    g.add((Game, HLTB[\"officialName\"], Literal(title, datatype=XSD.string)))\n",
    "\n",
    "    # Add multiplayer focus\n",
    "    g.add((Game, HLTB[\"multiplayerFocus\"],\n",
    "           Literal(pd.notnull(row[\"coop\"]) or pd.notnull(row[\"versus\"]), datatype=XSD.boolean)))\n",
    "\n",
    "    #Add hltb id\n",
    "    g.add((Game, HLTB[\"id\"], Literal(row[\"id\"], datatype=XSD.int)))\n",
    "\n",
    "    #Add hasGenre object property\n",
    "    for iterator in setGenreID(row[\"genres\"]):\n",
    "        if pd.notnull(iterator[1]) and iterator[1] != '':\n",
    "            g.add((Game, HLTB[\"hasGenre\"], URIRef(HLTB[iterator[1]])))\n",
    "\n",
    "    #Add developers\n",
    "    if pd.notna(row['developers']):\n",
    "        for iterator in row['developers'].split(', '):\n",
    "            get_string = lambda s: s.split('(')[0]\n",
    "            g.add((Game, HLTB['developedBy'], HLTB[setCompanyID(get_string(iterator))]))\n",
    "\n",
    "    #Add publishers\n",
    "    if pd.notna(row['publishers']):\n",
    "        for iterator in row['publishers'].split(', '):\n",
    "            get_string = lambda s: s.split('(')[0]\n",
    "            g.add((Game, HLTB['publishedBy'], HLTB[setCompanyID(get_string(iterator))]))\n",
    "\n",
    "    #Add platform availability\n",
    "    if pd.notna(row[\"platforms\"]):\n",
    "        for platform in row[\"platforms\"].split(\", \"):\n",
    "            g.add((Game, HLTB[\"releasedOn\"], URIRef(HLTB[setPlatformID(platform)])))\n",
    "\n",
    "            #Add Stats object property\n",
    "            g.add((Game, HLTB[\"hasStats\"],\n",
    "                   URIRef(HLTB[\"stats-\" + str(createGameID(title)) + \"___\" + str(setPlatformID(platform))])))\n",
    "\n",
    "            # Add Sales object property\n",
    "            game = merged.loc[(merged['title'] == title) & (merged['Platform'] == platform)]\n",
    "            if not game.empty:\n",
    "                if pd.notna(game[\"pal_sales\"].iloc[0]):\n",
    "                    GameSalesID = URIRef(\n",
    "                        HLTB[\"sales-\" + str(createGameID(title)) + \"___\" + str(setPlatformID(platform)) + \"___\" + \"eu\"])\n",
    "                    g.add((Game, HLTB[\"sold\"], GameSalesID))\n",
    "                if pd.notna(game[\"na_sales\"].iloc[0]):\n",
    "                    GameSalesID = URIRef(\n",
    "                        HLTB[\"sales-\" + str(createGameID(title)) + \"___\" + str(setPlatformID(platform)) + \"___\" + \"na\"])\n",
    "                    g.add((Game, HLTB[\"sold\"], GameSalesID))\n",
    "                if pd.notna(game[\"jp_sales\"].iloc[0]):\n",
    "                    GameSalesID = URIRef(\n",
    "                        HLTB[\"sales-\" + str(createGameID(title)) + \"___\" + str(setPlatformID(platform)) + \"___\" + \"jp\"])\n",
    "                    g.add((Game, HLTB[\"sold\"], GameSalesID))\n",
    "                if pd.notna(game[\"other_sales\"].iloc[0]):\n",
    "                    GameSalesID = URIRef(HLTB[\"sales-\" + str(createGameID(title)) + \"___\" + str(\n",
    "                        setPlatformID(platform)) + \"___\" + \"other\"])\n",
    "                    g.add((Game, HLTB[\"sold\"], GameSalesID))\n",
    "                if pd.notna(game[\"total_shipped\"].iloc[0]):\n",
    "                    GameSalesID = URIRef(HLTB[\"sales-\" + str(createGameID(title)) + \"___\" + str(\n",
    "                        setPlatformID(platform)) + \"___\" + \"global\"])\n",
    "                    g.add((Game, HLTB[\"sold\"], GameSalesID))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved games TTL file.\n"
     ]
    }
   ],
   "source": [
    "# Save the data in the Turtle format\n",
    "with open(gamesTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))\n",
    "\n",
    "print(\"Saved games TTL file.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Genre\n",
    "\n",
    "Now serializing the Genre class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()\n",
    "\n",
    "# Load the CSV files in memory\n",
    "genres = pd.read_csv(gamesPath, sep=\",\", index_col=\"genres\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "for genre, row in genres.iterrows():\n",
    "    allGenres = setGenreID(genre)\n",
    "    for iterator in allGenres:\n",
    "        if not (iterator[0] == \" \" or iterator[0] == \"\"):\n",
    "            Genre = URIRef(HLTB[iterator[1]])\n",
    "            #Add triples using store's add() method.\n",
    "            g.add((Genre, RDF.type, HLTB.Genre))\n",
    "            # Add the name of the genre\n",
    "            g.add((Genre, HLTB[\"name\"], Literal(iterator[0], datatype=XSD.string)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# Save genre data in the Turtle format\n",
    "with open(genresTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Platforms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()\n",
    "\n",
    "# Load the CSV files in memory\n",
    "platforms = pd.read_csv(platformsPath, sep=\",\", index_col=\"Platform\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "manufacturerDict = {}\n",
    "\n",
    "for platform, row in platforms.iterrows():\n",
    "    Platform = URIRef(HLTB[setPlatformID(platform)])\n",
    "    #Add triples using store's add() method.\n",
    "    g.add((Platform, RDF.type, HLTB.Platform))\n",
    "    # Add the name of the genre\n",
    "    g.add((Platform, HLTB[\"officialName\"], Literal(platform, datatype=XSD.string)))\n",
    "\n",
    "    #Add popularity if platform is popular\n",
    "    if pd.notna(row[\"Popular\"]):\n",
    "        g.add((Platform, HLTB[\"popular\"], Literal(True, datatype=XSD.boolean)))\n",
    "\n",
    "    #Add release date if present\n",
    "    if pd.notna(row[\"Release date\"]):\n",
    "        time = datetime.combine(datetime.strptime(row[\"Release date\"], '%Y-%M-%d'), datetime.min.time())\n",
    "        g.add((Platform, HLTB[\"releaseDate\"], Literal(time, datatype=XSD.dateTime)))\n",
    "\n",
    "    #Add CPU information\n",
    "    if pd.notna(row[\"CPU\"]):\n",
    "        g.add((Platform, HLTB[\"cpu\"], Literal(row[\"CPU\"], datatype=XSD.string)))\n",
    "\n",
    "    #Add CPU bit information\n",
    "    if pd.notna(row[\"\\\"Bits\\\"\"]):\n",
    "        bits = row[\"\\\"Bits\\\"\"].split(\"-\")[0]\n",
    "        g.add((Platform, HLTB[\"bits\"], Literal(bits, datatype=XSD.int)))\n",
    "\n",
    "    #Add acronym information\n",
    "    if pd.notna(row[\"Acronym\"]):\n",
    "        g.add((Platform, HLTB[\"acronym\"], Literal(row[\"Acronym\"], datatype=XSD.string)))\n",
    "\n",
    "    # add the manufacturer name\n",
    "    if pd.notna(row[\"Manufacturer\"]) or pd.notnull(row[\"Manufacturer\"]):\n",
    "        manufacturerStr = row[\"Manufacturer\"].strip()\n",
    "        if ',' in manufacturerStr:\n",
    "            manufacturerStr = manufacturerStr.replace(',', '/')\n",
    "        manufacturerSplit = manufacturerStr.split('/')\n",
    "        for elem in manufacturerSplit:\n",
    "            elem = elem.strip()\n",
    "            manufacturerName = ''\n",
    "            manufacturerCountry = ''\n",
    "            if '(' in elem:\n",
    "                elemSplit = elem.split('(')\n",
    "                manufacturerName = elemSplit[0].strip()\n",
    "                manufacturerCountry = elemSplit[1].strip()\n",
    "                if not manufacturerName[-1].isalnum():\n",
    "                    manufacturerName = manufacturerName[:-1]\n",
    "                if manufacturerCountry[-1] == ')':\n",
    "                    manufacturerCountry = manufacturerCountry[:-1]\n",
    "                manufacturerCountry = ''.join(c for c in manufacturerCountry if c.isalnum() or c == ' ')\n",
    "                if manufacturerCountry.lower() == 'south korea':\n",
    "                    manufacturerCountry = 'Korea, Republic of'\n",
    "                elif manufacturerCountry.lower() == 'us':\n",
    "                    manufacturerCountry = 'United States of America'\n",
    "            else:\n",
    "                manufacturerName = elem\n",
    "                if manufacturerName == 'Panasonic' or manufacturerName == 'Sega':\n",
    "                    manufacturerCountry = 'Japan'\n",
    "\n",
    "            manufacturerDict[manufacturerName] = manufacturerCountry\n",
    "            if manufacturerName != '':\n",
    "                # get manufacturer URI\n",
    "                manufacturerURI = URIRef(HLTB[setCompanyID(manufacturerName)])\n",
    "\n",
    "                # Add platform's manufacturer\n",
    "                g.add((Platform, HLTB[\"createdBy\"], manufacturerURI))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# Save data in the Turtle format\n",
    "with open(platformsTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stats\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()\n",
    "\n",
    "# Load the CSV files in memory\n",
    "games = pd.read_csv(gamesPath, sep=\",\")\n",
    "vgchartz = pd.read_csv(vgchartzPath, sep=\",\")\n",
    "completionTime = pd.read_csv(completionTimePath, sep=\",\")\n",
    "platforms = pd.read_csv(platformsPath, sep=\",\")\n",
    "\n",
    "# Create merged datasets\n",
    "merged_temp = pd.merge(games, completionTime, left_on='id', right_on='gameID')\n",
    "merged_temp1 = pd.merge(vgchartz, platforms, left_on='console', right_on='Acronym', how='left')\n",
    "merged = pd.merge(merged_temp, merged_temp1, left_on=[\"title\", \"platform\"], right_on=[\"title\", \"Platform\"], how='left')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "for id, row in merged.iterrows():\n",
    "    if pd.notna(row[\"title\"]) and pd.notna(row[\"platform\"]):\n",
    "        StatsID = URIRef(HLTB[createGameID(\"stats-\" + str(createGameID(row[\"title\"]))) + \"___\" + str(\n",
    "            setPlatformID(row[\"platform\"]))])\n",
    "\n",
    "        #Adding node type\n",
    "        g.add((StatsID, HLTB.Type, HLTB.Stats))\n",
    "\n",
    "        #Add Time information\n",
    "        if pd.notna(row[\"count_comp\"]) and int(row[\"count_comp\"]) > 1:\n",
    "            g.add((StatsID, HLTB[\"polledTime\"], Literal(int(row[\"count_comp\"]), datatype=XSD.int)))\n",
    "            g.add((StatsID, HLTB[\"mainTime\"], Literal(int(row[\"comp_main\"]), datatype=XSD.int)))\n",
    "            g.add((StatsID, HLTB[\"mainPlusTime\"], Literal(int(row[\"comp_plus\"]), datatype=XSD.int)))\n",
    "            g.add((StatsID, HLTB[\"completionistTime\"], Literal(int(row[\"comp_100\"]), datatype=XSD.int)))\n",
    "            g.add((StatsID, HLTB[\"slowestTime\"], Literal(int(row[\"comp_high\"]), datatype=XSD.int)))\n",
    "            g.add((StatsID, HLTB[\"fastestTime\"], Literal(int(row[\"comp_low\"]), datatype=XSD.int)))\n",
    "\n",
    "        #Add remaining stats\n",
    "        if pd.notna(row[\"critic_score\"]) and (float(row[\"critic_score\"]) > 0.1):\n",
    "            g.add((StatsID, HLTB[\"criticScore\"], Literal(row[\"critic_score\"], datatype=XSD.float)))\n",
    "        if pd.notna(row[\"user_score\"]) and (float(row[\"user_score\"]) > 0.1):\n",
    "            g.add((StatsID, HLTB[\"userScore\"], Literal(row[\"user_score\"], datatype=XSD.float)))\n",
    "\n",
    "        if pd.notna(row[\"release_date\"]):\n",
    "            time = datetime.combine(datetime.strptime(row[\"release_date\"], '%Y-%M-%d'), datetime.min.time())\n",
    "            g.add((StatsID, HLTB[\"releaseDate\"], Literal(time, datatype=XSD.dateTime)))\n",
    "\n",
    "        # Add \"onPlatform\" object property\n",
    "        g.add((StatsID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"platform\"])])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved stats TTL file.\n"
     ]
    }
   ],
   "source": [
    "# Save the data in the Turtle format\n",
    "with open(statsTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))\n",
    "\n",
    "print(\"Saved stats TTL file.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Game sales"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()\n",
    "\n",
    "# Load the CSV files in memory\n",
    "vgchartz = pd.read_csv(vgchartzPath, sep=\",\")\n",
    "platforms = pd.read_csv(platformsPath, sep=\",\")\n",
    "\n",
    "# Create merged datasets\n",
    "merged = pd.merge(vgchartz, platforms, left_on='console', right_on='Acronym', how='left')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "for id, row in merged.iterrows():\n",
    "    if pd.notna(row[\"Platform\"]):\n",
    "\n",
    "        if pd.notna(row[\"pal_sales\"]) and (row[\"pal_sales\"] > 0):\n",
    "            GameSalesID = URIRef(HLTB[\"sales-\" + str(createGameID(row[\"title\"])) + \"___\" + str(\n",
    "                setPlatformID(row[\"Platform\"])) + \"___\" + \"eu\"])\n",
    "            g.add((GameSalesID, HLTB.Type, HLTB.Sale))\n",
    "            g.add((GameSalesID, HLTB[\"unitsSold\"], Literal(row[\"pal_sales\"], datatype=XSD.float)))\n",
    "            g.add((GameSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"eu\"])))\n",
    "            g.add((GameSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n",
    "\n",
    "        if pd.notna(row[\"jp_sales\"]) and (row[\"jp_sales\"] > 0):\n",
    "            GameSalesID = URIRef(HLTB[\"sales-\" + str(createGameID(row[\"title\"])) + \"___\" + str(\n",
    "                setPlatformID(row[\"Platform\"])) + \"___\" + \"jp\"])\n",
    "            g.add((GameSalesID, HLTB.Type, HLTB.Sale))\n",
    "            g.add((GameSalesID, HLTB[\"unitsSold\"], Literal(row[\"jp_sales\"], datatype=XSD.float)))\n",
    "            g.add((GameSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"jp\"])))\n",
    "            g.add((GameSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n",
    "\n",
    "        if pd.notna(row[\"na_sales\"]) and (row[\"na_sales\"] > 0):\n",
    "            GameSalesID = URIRef(HLTB[\"sales-\" + str(createGameID(row[\"title\"])) + \"___\" + str(\n",
    "                setPlatformID(row[\"Platform\"])) + \"___\" + \"na\"])\n",
    "            g.add((GameSalesID, HLTB.Type, HLTB.Sale))\n",
    "            g.add((GameSalesID, HLTB[\"unitsSold\"], Literal(row[\"na_sales\"], datatype=XSD.float)))\n",
    "            g.add((GameSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"na\"])))\n",
    "            g.add((GameSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n",
    "\n",
    "        if pd.notna(row[\"other_sales\"]) and (row[\"other_sales\"] > 0):\n",
    "            GameSalesID = URIRef(HLTB[\"sales-\" + str(createGameID(row[\"title\"])) + \"___\" + str(\n",
    "                setPlatformID(row[\"Platform\"])) + \"___\" + \"other\"])\n",
    "            g.add((GameSalesID, HLTB.Type, HLTB.Sale))\n",
    "            g.add((GameSalesID, HLTB[\"unitsSold\"], Literal(row[\"other_sales\"], datatype=XSD.float)))\n",
    "            g.add((GameSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"other\"])))\n",
    "            g.add((GameSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n",
    "\n",
    "        if pd.notna(row[\"total_shipped\"]) and (row[\"total_shipped\"] > 0):\n",
    "            GameSalesID = URIRef(HLTB[\"sales-\" + str(createGameID(row[\"title\"])) + \"___\" + str(\n",
    "                setPlatformID(row[\"Platform\"])) + \"___\" + \"global\"])\n",
    "            g.add((GameSalesID, HLTB.Type, HLTB.Sale))\n",
    "            g.add((GameSalesID, HLTB[\"unitsSold\"], Literal(row[\"total_shipped\"], datatype=XSD.float)))\n",
    "            g.add((GameSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"global\"])))\n",
    "            g.add((GameSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved game-sales TTL file.\n"
     ]
    }
   ],
   "source": [
    "# Save the data in the Turtle format\n",
    "with open(gameSalesTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))\n",
    "\n",
    "print(\"Saved game-sales TTL file.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Companies\n",
    "\n",
    "### indie-game-developers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()\n",
    "\n",
    "# Load the CSV files in memory\n",
    "indiegamesdevelopers = pd.read_csv(indiegamesdevelopersPath, sep=\",\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "for index, row in indiegamesdevelopers.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    Company = URIRef(HLTB[setCompanyID(row['Developer'])])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Company, RDF.type, HLTB.Company))\n",
    "\n",
    "    # Add the Company\n",
    "    g.add((Company, HLTB[\"indieDeveloper\"], Literal(pd.notnull(row['Developer']), datatype=XSD.boolean)))\n",
    "    g.add((Company, HLTB['officialName'], Literal(row['Developer'], datatype=XSD.string)))\n",
    "\n",
    "    # Add the company's country\n",
    "    code = getCountry2Digits(row['Country'])\n",
    "    if code != '':\n",
    "        g.add((Company, HLTB['basedIn'], CNS[code]))\n",
    "\n",
    "    # Add the notable games\n",
    "    if pd.notnull(row['selected_titles_series']) or pd.notna(row['selected_titles_series']):\n",
    "        g.add((Company, HLTB['hasNotableGame'], HLTB[createGameID(row['selected_titles_series'])]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Video-game-developers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# Create merged datasets\n",
    "videoGameDevelopers = pd.read_csv(videoGameDevelopersPath, sep=\",\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "for index, row in videoGameDevelopers.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    Company = URIRef(HLTB[setCompanyID(row['Developer'])])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Company, RDF.type, HLTB.Company))\n",
    "\n",
    "    # Add the Company\n",
    "    g.add((Company, HLTB['officialName'], Literal(row['Developer'], datatype=XSD.string)))\n",
    "\n",
    "    # Add the country\n",
    "    code = getCountry2Digits(row['Country'])\n",
    "    if code != '':\n",
    "        g.add((Company, HLTB['basedIn'], CNS[code]))\n",
    "\n",
    "    # Add the notable games\n",
    "    if pd.notnull(row['selected_titles_series']) or pd.notna(row['selected_titles_series']):\n",
    "        g.add((Company, HLTB['hasNotableGame'], HLTB[createGameID(row['selected_titles_series'])]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Games"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# Iterate over the games\n",
    "for title, row in games.iterrows():\n",
    "    if pd.notna(row['developers']):\n",
    "        for iterator in row['developers'].split(', '):\n",
    "            get_string = lambda s: s.split('(')[0]\n",
    "            # Add company\n",
    "            Company = URIRef(HLTB[setCompanyID(get_string(iterator))])\n",
    "            g.add((Company, RDF.type, HLTB.Company))\n",
    "\n",
    "    #Add publishers\n",
    "    if pd.notna(row['publishers']):\n",
    "        for iterator in row['publishers'].split(', '):\n",
    "            get_string = lambda s: s.split('(')[0]\n",
    "            # Add company\n",
    "            Company = URIRef(HLTB[setCompanyID(get_string(iterator))])\n",
    "            g.add((Company, RDF.type, HLTB.Company))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Manufacturers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "for manufacturer, country in manufacturerDict.items():\n",
    "    manufacturerURI = URIRef(HLTB[setCompanyID(manufacturer)])\n",
    "    g.add((manufacturerURI, RDF.type, HLTB.Company))\n",
    "    g.add((manufacturerURI, HLTB['officialName'], Literal(manufacturer, datatype=XSD.string)))\n",
    "    # Add manufacturer's country\n",
    "    g.add((manufacturerURI, HLTB['basedIn'], CNS[getCountry2Digits(country)]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved company TTL file.\n"
     ]
    }
   ],
   "source": [
    "# Save the data in the Turtle format\n",
    "with open(companiesTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))\n",
    "\n",
    "print(\"Saved company TTL file.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Country"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()\n",
    "\n",
    "# Create merged datasets\n",
    "rawCountriesRegions = pd.read_csv(rawCountriesRegionsPath, sep=\",\", index_col='name')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "countriesRegions = rawCountriesRegions.replace(to_replace=['Asia', 'Europe', 'Africa', 'Oceania', 'Americas', 'NaN'],\n",
    "                                               value=['other', 'eu', 'other', 'other', 'other', 'other'])\n",
    "northAmericaCountries = ['ai', 'ag', 'aw', 'ag', 'aw', 'bs', 'bb', 'bz', 'bm', 'bq', 'vg', 'ca', 'ky', 'cr', 'cu', 'cw',\n",
    "                         'dm', 'do', 'sv', 've', 'gl', 'gd', 'gp', 'gt', 'ht', 'hn', 'jm', 'mq', 'mx', 'ms', 'ni', 'pa',\n",
    "                         'pr', 'bl', 'kn', 'lc', 'mf', 'pm', 'vc', 'sx', 'tt', 'tc', 'us', 'vi']\n",
    "for index, row in countriesRegions.iterrows():\n",
    "    code = getCountry2Digits(index)\n",
    "    region = row['region']\n",
    "    if code == 'jp':\n",
    "        region = 'jp'\n",
    "    elif code == 'aq':\n",
    "        region = 'other'\n",
    "    elif code in northAmericaCountries:\n",
    "        region = 'na'\n",
    "    g.add((CNS[code], HLTB['locatedIn'], HLTB[region]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Countries TTL file.\n"
     ]
    }
   ],
   "source": [
    "# Save the data in the Turtle format\n",
    "with open(countriesTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))\n",
    "\n",
    "print(\"Saved Countries TTL file.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Platform sales"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "g = createGraph()\n",
    "platforms = pd.read_csv(platformsPath, sep=\",\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "for id, row in platforms.iterrows():\n",
    "    if pd.notna(row[\"Platform\"]):\n",
    "\n",
    "        if pd.notna(row[\"Europe\"]) and (row[\"Europe\"] > 0):\n",
    "            PlatformSalesID = URIRef(HLTB[\"sales-\" + str(setPlatformID(row[\"Platform\"])) + \"___\" + \"eu\"])\n",
    "            g.add((PlatformSalesID, HLTB.Type, HLTB.Sale))\n",
    "            g.add((PlatformSalesID, HLTB[\"unitsSold\"], Literal(row[\"Europe\"], datatype=XSD.float)))\n",
    "            g.add((PlatformSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"eu\"])))\n",
    "            g.add((PlatformSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n",
    "\n",
    "        if pd.notna(row[\"Japan\"]) and (row[\"Japan\"] > 0):\n",
    "            PlatformSalesID = URIRef(HLTB[\"sales-\" + str(setPlatformID(row[\"Platform\"])) + \"___\" + \"jp\"])\n",
    "\n",
    "            g.add((PlatformSalesID, HLTB.Type, HLTB.Sale))\n",
    "            g.add((PlatformSalesID, HLTB[\"unitsSold\"], Literal(row[\"Japan\"], datatype=XSD.float)))\n",
    "            g.add((PlatformSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"jp\"])))\n",
    "            g.add((PlatformSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n",
    "\n",
    "        if pd.notna(row[\"North America\"]) and (row[\"Japan\"] > 0):\n",
    "            PlatformSalesID = URIRef(HLTB[\"sales-\" + str(setPlatformID(row[\"Platform\"])) + \"___\" + \"na\"])\n",
    "\n",
    "            g.add((PlatformSalesID, HLTB.Type, HLTB.Sale))\n",
    "            g.add((PlatformSalesID, HLTB[\"unitsSold\"], Literal(row[\"North America\"], datatype=XSD.float)))\n",
    "            g.add((PlatformSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"na\"])))\n",
    "            g.add((PlatformSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n",
    "\n",
    "        if pd.notna(row[\"Rest of World\"]) and (row[\"Rest of World\"] > 0):\n",
    "            PlatformSalesID = URIRef(HLTB[\"sales-\" + str(setPlatformID(row[\"Platform\"])) + \"___\" + \"other\"])\n",
    "\n",
    "            g.add((PlatformSalesID, HLTB.Type, HLTB.Sale))\n",
    "            g.add((PlatformSalesID, HLTB[\"unitsSold\"], Literal(row[\"Rest of World\"], datatype=XSD.float)))\n",
    "            g.add((PlatformSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"other\"])))\n",
    "            g.add((PlatformSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))\n",
    "\n",
    "        if pd.notna(row[\"Global\"]) and (row[\"Global\"] > 0):\n",
    "            PlatformSalesID = URIRef(HLTB[\"sales-\" + str(setPlatformID(row[\"Platform\"])) + \"___\" + \"global\"])\n",
    "            g.add((PlatformSalesID, HLTB.Type, HLTB.Sale))\n",
    "            g.add((PlatformSalesID, HLTB[\"unitsSold\"], Literal(row[\"Global\"], datatype=XSD.float)))\n",
    "            g.add((PlatformSalesID, HLTB[\"locatedIn\"], URIRef(HLTB[\"global\"])))\n",
    "            g.add((PlatformSalesID, HLTB[\"onPlatform\"], URIRef(HLTB[setPlatformID(row[\"Platform\"])])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved platforms Sales TTL file.\n"
     ]
    }
   ],
   "source": [
    "# Save the data in the Turtle format\n",
    "with open(platformsSalesTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))\n",
    "\n",
    "print(\"Saved platforms Sales TTL file.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
